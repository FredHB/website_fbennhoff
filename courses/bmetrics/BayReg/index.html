
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../">
      
      
        <link rel="next" href="../UCSV/">
      
      
      <link rel="icon" href="../../../assets/images/logo.jpg">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.5.32">
    
    
      
        <title>1 Bayesian Regression - Frederik H Bennhoff <br><small><i>[public] finance | macroeconomics</i></small></title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.3cba04c6.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=IBM+Plex+Serif:300,300i,400,400i,700,700i%7CIBM+Plex+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"IBM Plex Serif";--md-code-font:"IBM Plex Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#bayesian-regression" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="Frederik H Bennhoff &lt;br&gt;&lt;small&gt;&lt;i&gt;[public] finance | macroeconomics&lt;/i&gt;&lt;/small&gt;" class="md-header__button md-logo" aria-label="Frederik H Bennhoff <br><small><i>[public] finance | macroeconomics</i></small>" data-md-component="logo">
      
  <img src="../../../assets/images/logo.jpg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Frederik H Bennhoff <br><small><i>[public] finance | macroeconomics</i></small>
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              1 Bayesian Regression
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Frederik H Bennhoff &lt;br&gt;&lt;small&gt;&lt;i&gt;[public] finance | macroeconomics&lt;/i&gt;&lt;/small&gt;" class="md-nav__button md-logo" aria-label="Frederik H Bennhoff <br><small><i>[public] finance | macroeconomics</i></small>" data-md-component="logo">
      
  <img src="../../../assets/images/logo.jpg" alt="logo">

    </a>
    Frederik H Bennhoff <br><small><i>[public] finance | macroeconomics</i></small>
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    about me
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../cv/cv/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cv
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../research/research/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    research
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    short courses & resources
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            short courses & resources
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../sem/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    solving economic models
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_4_1" id="__nav_4_1_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_1">
            <span class="md-nav__icon md-icon"></span>
            solving economic models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../sem/1-numpy/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1 Numpy
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../sem/2-numba/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2 Numba
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../sem/3-application_stoch_proc/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    3 Application: Stochastic Processes
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../sem/4-optimization/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    4 Optimization
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../sem/5-endogenous_grid_method/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    5 Endogenous Grid Method
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_2" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    bayesian econometrics
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_4_2" id="__nav_4_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4_2">
            <span class="md-nav__icon md-icon"></span>
            bayesian econometrics
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    1 Bayesian Regression
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    1 Bayesian Regression
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#writing-down-a-bayesian-regression-model" class="md-nav__link">
    <span class="md-ellipsis">
      Writing down a Bayesian Regression Model
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Writing down a Bayesian Regression Model">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-natural-conjugate-prior" class="md-nav__link">
    <span class="md-ellipsis">
      The Natural Conjugate Prior
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prediction-and-monte-carlo-integration" class="md-nav__link">
    <span class="md-ellipsis">
      Prediction and Monte Carlo Integration
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#independent-normal-gamma-priors-and-heteroskedastic-errors" class="md-nav__link">
    <span class="md-ellipsis">
      Independent Normal-Gamma Priors and Heteroskedastic Errors
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Independent Normal-Gamma Priors and Heteroskedastic Errors">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#model-set-up" class="md-nav__link">
    <span class="md-ellipsis">
      Model Set-Up
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hyperparameters-and-m-h-random-walk" class="md-nav__link">
    <span class="md-ellipsis">
      Hyperparameters and M-H Random Walk
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#coding-the-full-model" class="md-nav__link">
    <span class="md-ellipsis">
      Coding the Full Model
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gibbs-sampler" class="md-nav__link">
    <span class="md-ellipsis">
      Gibbs sampler
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#parallel-gibbs-sampler" class="md-nav__link">
    <span class="md-ellipsis">
      Parallel Gibbs Sampler
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Parallel Gibbs Sampler">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#visualization" class="md-nav__link">
    <span class="md-ellipsis">
      Visualization
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#a-full-class-bayesianregressioninghet" class="md-nav__link">
    <span class="md-ellipsis">
      A Full Class BayesianRegressionIngHet
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#applications" class="md-nav__link">
    <span class="md-ellipsis">
      Applications
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Applications">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#house-prices" class="md-nav__link">
    <span class="md-ellipsis">
      House Prices
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#is-real-gdp-oscillatory-andor-explosive" class="md-nav__link">
    <span class="md-ellipsis">
      Is Real GDP Oscillatory and/or Explosive?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../UCSV/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2 The Unobserved Component Stochastic Volatility Model
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../TVP-AR/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    3 The Time-Varying Parameter Model
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#writing-down-a-bayesian-regression-model" class="md-nav__link">
    <span class="md-ellipsis">
      Writing down a Bayesian Regression Model
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Writing down a Bayesian Regression Model">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-natural-conjugate-prior" class="md-nav__link">
    <span class="md-ellipsis">
      The Natural Conjugate Prior
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prediction-and-monte-carlo-integration" class="md-nav__link">
    <span class="md-ellipsis">
      Prediction and Monte Carlo Integration
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#independent-normal-gamma-priors-and-heteroskedastic-errors" class="md-nav__link">
    <span class="md-ellipsis">
      Independent Normal-Gamma Priors and Heteroskedastic Errors
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Independent Normal-Gamma Priors and Heteroskedastic Errors">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#model-set-up" class="md-nav__link">
    <span class="md-ellipsis">
      Model Set-Up
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hyperparameters-and-m-h-random-walk" class="md-nav__link">
    <span class="md-ellipsis">
      Hyperparameters and M-H Random Walk
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#coding-the-full-model" class="md-nav__link">
    <span class="md-ellipsis">
      Coding the Full Model
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gibbs-sampler" class="md-nav__link">
    <span class="md-ellipsis">
      Gibbs sampler
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#parallel-gibbs-sampler" class="md-nav__link">
    <span class="md-ellipsis">
      Parallel Gibbs Sampler
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Parallel Gibbs Sampler">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#visualization" class="md-nav__link">
    <span class="md-ellipsis">
      Visualization
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#a-full-class-bayesianregressioninghet" class="md-nav__link">
    <span class="md-ellipsis">
      A Full Class BayesianRegressionIngHet
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#applications" class="md-nav__link">
    <span class="md-ellipsis">
      Applications
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Applications">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#house-prices" class="md-nav__link">
    <span class="md-ellipsis">
      House Prices
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#is-real-gdp-oscillatory-andor-explosive" class="md-nav__link">
    <span class="md-ellipsis">
      Is Real GDP Oscillatory and/or Explosive?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="bayesian-regression"><strong><em>Bayesian Regression</em></strong><a class="headerlink" href="#bayesian-regression" title="Permanent link">#</a></h1>
<p>In this notebook we write a module suitable for two essential <strong>Bayesian Regression</strong> models. A linear regression model with the natural conjugate prior and a linear regression model with the independent Normal-Gamma prior over regression coefficients <span class="arithmatex"><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span> and regression precision (inverse standard deviation), <span class="arithmatex"><span class="MathJax_Preview">h</span><script type="math/tex">h</script></span>.</p>
<p>Here is a list of functionalities we wish to implement in this course:</p>
<ol>
<li>Given a dataset <span class="arithmatex"><span class="MathJax_Preview">(y, X)</span><script type="math/tex">(y, X)</script></span>, calculate the posterior distribution of <span class="arithmatex"><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span> for the natural conjugate and the independent NG prior.</li>
<li>To do (1), implement a Gibbs sampler of the posterior for any function <span class="arithmatex"><span class="MathJax_Preview">f(\beta | y)</span><script type="math/tex">f(\beta | y)</script></span> since the NG prior cannot be handled analytically</li>
<li>Since analytical results for the natural conjugate are possible, include methods to obtain these</li>
<li>Include prediction methods for <span class="arithmatex"><span class="MathJax_Preview">\hat y |y, X</span><script type="math/tex">\hat y |y, X</script></span>.</li>
<li>Allow for heteroskedasticity in the errors.</li>
</ol>
<h2 id="writing-down-a-bayesian-regression-model"><strong>Writing down a Bayesian Regression Model</strong><a class="headerlink" href="#writing-down-a-bayesian-regression-model" title="Permanent link">#</a></h2>
<p>The baseline model is (<span class="arithmatex"><span class="MathJax_Preview">\mathrm{dim}(\beta) = k</span><script type="math/tex">\mathrm{dim}(\beta) = k</script></span>) </p>
<div class="arithmatex">
<div class="MathJax_Preview">
    y = X\beta + w, w \sim N(0_N, h^{-1} I_N)
</div>
<script type="math/tex; mode=display">
    y = X\beta + w, w \sim N(0_N, h^{-1} I_N)
</script>
</div>
<p>where by convention, <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> first column are ones. We use the model from <em>Bayesian Econometric Methods</em> by Gary Koop, Dale J. Poirier, and Justin L. Tobias.</p>
<p>We make the following definitions, where <span class="arithmatex"><span class="MathJax_Preview">\nu</span><script type="math/tex">\nu</script></span> is the degrees of freedom, <span class="arithmatex"><span class="MathJax_Preview">\hat \beta</span><script type="math/tex">\hat \beta</script></span> is the OLS estimate of <span class="arithmatex"><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span> and <span class="arithmatex"><span class="MathJax_Preview">s^2</span><script type="math/tex">s^2</script></span> is the OLS estimate of the variance: </p>
<div class="arithmatex">
<div class="MathJax_Preview"> 
\begin{align*}
    \nu = N - k \\
    \hat{\beta} = (X'X)^{-1} X'y, \\
    s^2 = \frac{(y - X\hat{\beta})'(y - X\hat{\beta})}{\nu}.
\end{align*}
</div>
<script type="math/tex; mode=display"> 
\begin{align*}
    \nu = N - k \\
    \hat{\beta} = (X'X)^{-1} X'y, \\
    s^2 = \frac{(y - X\hat{\beta})'(y - X\hat{\beta})}{\nu}.
\end{align*}
</script>
</div>
<p>Then, the likelihood of the data given <span class="arithmatex"><span class="MathJax_Preview">h, \beta</span><script type="math/tex">h, \beta</script></span> is</p>
<div class="arithmatex">
<div class="MathJax_Preview">
    p(y|\beta, h) = \frac{1}{(2\pi)^{K/2}}
    \left\{
    h^{k/2} \exp\left[-\frac{h}{2} (\beta - \hat{\beta})' X'X (\beta - \hat{\beta})\right]
    \right\}
    \left\{
    h^{\nu/2} \exp\left[-\frac{h \nu}{2 s^2}\right]
    \right\} \\
    \propto \left\{
    h^{k/2} \exp\left[-\frac{h}{2} (\beta - \hat{\beta})' X'X (\beta - \hat{\beta})\right]
    \right\}
    \left\{
    h^{\nu/2} \exp\left[-\frac{h \nu}{2 s^2}\right]
    \right\}.
</div>
<script type="math/tex; mode=display">
    p(y|\beta, h) = \frac{1}{(2\pi)^{K/2}}
    \left\{
    h^{k/2} \exp\left[-\frac{h}{2} (\beta - \hat{\beta})' X'X (\beta - \hat{\beta})\right]
    \right\}
    \left\{
    h^{\nu/2} \exp\left[-\frac{h \nu}{2 s^2}\right]
    \right\} \\
    \propto \left\{
    h^{k/2} \exp\left[-\frac{h}{2} (\beta - \hat{\beta})' X'X (\beta - \hat{\beta})\right]
    \right\}
    \left\{
    h^{\nu/2} \exp\left[-\frac{h \nu}{2 s^2}\right]
    \right\}.
</script>
</div>
<p><strong><em>Task [concentrated likelihood]:</em></strong> </p>
<p>Write a function to compute the concentrated log-likelihood for <span class="arithmatex"><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span> without the scaling term. One obtains a concentrated likelihood by substuting out other parameters for their maximum-likelihood estimate.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">numba</span> <span class="kn">import</span> <span class="n">jit</span><span class="p">,</span> <span class="n">njit</span><span class="p">,</span> <span class="n">prange</span>
<span class="c1"># simulate some data</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span> <span class="c1"># intercept</span>
<span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">beta</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</code></pre></div>
<p><strong><em>Solution [concentrated likelihood]</em></strong>: </p>
<p>First, note that the MLE of <span class="arithmatex"><span class="MathJax_Preview">\sigma^2 = h^{-1}</span><script type="math/tex">\sigma^2 = h^{-1}</script></span> is <span class="arithmatex"><span class="MathJax_Preview">s^2</span><script type="math/tex">s^2</script></span>. Find the MLE of  <span class="arithmatex"><span class="MathJax_Preview">\sigma^2</span><script type="math/tex">\sigma^2</script></span>  conditional on  <span class="arithmatex"><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span> :</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\sigma}^2(\beta) = \frac{(y - X\beta){\prime}(y - X\beta)}{N}.
</div>
<script type="math/tex; mode=display">
\hat{\sigma}^2(\beta) = \frac{(y - X\beta){\prime}(y - X\beta)}{N}.
</script>
</div>
<p>Now, substitute <span class="arithmatex"><span class="MathJax_Preview">\hat{\sigma}^2(\beta)</span><script type="math/tex">\hat{\sigma}^2(\beta)</script></span> back into the likelihood:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
    p(y | \beta) = \frac{1}{(2\pi)^{N/2}} \left(\frac{1}{\hat{\sigma}^2(\beta)}\right)^{N/2} \exp\left(-\frac{1}{2\hat{\sigma}^2(\beta)} (y - X\beta){\prime}(y - X\beta)\right).
</div>
<script type="math/tex; mode=display">
    p(y | \beta) = \frac{1}{(2\pi)^{N/2}} \left(\frac{1}{\hat{\sigma}^2(\beta)}\right)^{N/2} \exp\left(-\frac{1}{2\hat{\sigma}^2(\beta)} (y - X\beta){\prime}(y - X\beta)\right).
</script>
</div>
<p>Then 
$$
    p(y | \beta) \propto \left((y - X\beta)^{\prime}(y - X\beta)\right)^{-N/2}.
$$</p>
<div class="highlight"><pre><span></span><code><span class="nd">@jit</span><span class="p">(</span><span class="n">nopython</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">ols_solution</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">XX</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span>
    <span class="n">nu</span> <span class="o">=</span> <span class="n">N</span> <span class="o">-</span> <span class="n">K</span>
    <span class="n">beta_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">XX</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">s_sq</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">nu</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">X</span> <span class="o">@</span> <span class="n">beta_hat</span><span class="p">)</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">X</span> <span class="o">@</span> <span class="n">beta_hat</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">beta_hat</span><span class="p">,</span> <span class="n">s_sq</span><span class="p">,</span> <span class="n">nu</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">XX</span>


<span class="nd">@jit</span><span class="p">(</span><span class="n">nopython</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">clhood_beta</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">ssr</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">X</span> <span class="o">@</span> <span class="n">beta</span><span class="p">)</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">X</span> <span class="o">@</span> <span class="n">beta</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">ssr</span><span class="p">))</span><span class="o">*</span><span class="p">(</span><span class="o">-</span><span class="n">N</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="c1"># try it out</span>
<span class="n">beta_hat</span><span class="p">,</span> <span class="n">s_sq</span><span class="p">,</span> <span class="n">nu</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">XX</span> <span class="o">=</span> <span class="n">ols_solution</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;ols beta_hat: </span><span class="si">{</span><span class="n">beta_hat</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">clhood_beta</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;concentrated log-likelihood: </span><span class="si">{</span><span class="n">clhood_beta</span><span class="p">(</span><span class="n">beta_hat</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">X</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>ols beta_hat: [0.86907181 2.14332413 3.06126176]
concentrated log-likelihood: -3450.6575608188514
</code></pre></div>
<h3 id="the-natural-conjugate-prior"><strong>The Natural Conjugate Prior</strong><a class="headerlink" href="#the-natural-conjugate-prior" title="Permanent link">#</a></h3>
<p>We begin with a natural normal-gamma conjugate prior. This means:</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span> conditional on <span class="arithmatex"><span class="MathJax_Preview">h</span><script type="math/tex">h</script></span> is multivariate Normal:</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">\beta | h \sim N(\underline\beta, h^{-1} \underline V)</div>
<script type="math/tex; mode=display">\beta | h \sim N(\underline\beta, h^{-1} \underline V)</script>
</div>
<ul>
<li>Prior for error precision, <span class="arithmatex"><span class="MathJax_Preview">h</span><script type="math/tex">h</script></span>, is Gamma:  </li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">h \sim G(\underline s^{-2}, \underline \nu)</div>
<script type="math/tex; mode=display">h \sim G(\underline s^{-2}, \underline \nu)</script>
</div>
<p>Note that we parametrize the Gamma distribution in terms of mean and degrees of freedom: <span class="arithmatex"><span class="MathJax_Preview">\underline s^{-2}</span><script type="math/tex">\underline s^{-2}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\underline \nu</span><script type="math/tex">\underline \nu</script></span>, respectively.</p>
<ul>
<li>One writes for Normal-Gamma distribution:</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">\beta, h \sim NG\left(\underline \beta, \underline V, \underline s^{-2}, \underline \nu\right)</div>
<script type="math/tex; mode=display">\beta, h \sim NG\left(\underline \beta, \underline V, \underline s^{-2}, \underline \nu\right)</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\underline \beta</span><script type="math/tex">\underline \beta</script></span>, <span class="arithmatex"><span class="MathJax_Preview">\underline V</span><script type="math/tex">\underline V</script></span>, <span class="arithmatex"><span class="MathJax_Preview">\underline s^{-2}</span><script type="math/tex">\underline s^{-2}</script></span>, and <span class="arithmatex"><span class="MathJax_Preview">\underline \nu</span><script type="math/tex">\underline \nu</script></span> are prior hyperparameter values chosen by the researcher.</li>
</ul>
<p>The posterior for the NG-prior is:</p>
<div class="arithmatex">
<div class="MathJax_Preview">\beta, h | y \sim NG\left(\overline{\beta}, \overline{V}, \overline{s}^{-2}, \overline{\nu}\right),</div>
<script type="math/tex; mode=display">\beta, h | y \sim NG\left(\overline{\beta}, \overline{V}, \overline{s}^{-2}, \overline{\nu}\right),</script>
</div>
<p>where: </p>
<div class="arithmatex">
<div class="MathJax_Preview">\begin{align*}\overline{V} &amp;= \left(\underline V^{-1} + X'X\right)^{-1}, \\
  \overline{\beta} &amp;= \overline{V} \left(\underline V^{-1}\beta + X'X\hat{\beta}\right), \\
  \overline{\nu} &amp;= \underline\nu + N
  \end{align*}
</div>
<script type="math/tex; mode=display">\begin{align*}\overline{V} &= \left(\underline V^{-1} + X'X\right)^{-1}, \\
  \overline{\beta} &= \overline{V} \left(\underline V^{-1}\beta + X'X\hat{\beta}\right), \\
  \overline{\nu} &= \underline\nu + N
  \end{align*}
</script>
</div>
<p>And <span class="arithmatex"><span class="MathJax_Preview">\overline{s}^{-2}</span><script type="math/tex">\overline{s}^{-2}</script></span> is defined through:  </p>
<div class="arithmatex">
<div class="MathJax_Preview">\overline{s}^2 = \frac{\underline\nu \underline s^2 + \nu s^2 + (\hat{\beta} - \beta)' \left[\underline V + (X'X)^{-1}\right]^{-1} (\hat{\beta} - \beta)}{\overline{\nu}}.</div>
<script type="math/tex; mode=display">\overline{s}^2 = \frac{\underline\nu \underline s^2 + \nu s^2 + (\hat{\beta} - \beta)' \left[\underline V + (X'X)^{-1}\right]^{-1} (\hat{\beta} - \beta)}{\overline{\nu}}.</script>
</div>
<p><strong><em>Task [natural conjugate posterior]:</em></strong></p>
<p>Write a function to obtain the posterior parameters <span class="arithmatex"><span class="MathJax_Preview">\overline V, \overline \beta, \overline \nu, \overline{s}^2</span><script type="math/tex">\overline V, \overline \beta, \overline \nu, \overline{s}^2</script></span></p>
<p><strong>Definition (gamma distribution).</strong></p>
<p>A continuous random variable <span class="arithmatex"><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span> has a <strong>gamma distribution</strong> with mean <span class="arithmatex"><span class="MathJax_Preview">\mu &gt; 0</span><script type="math/tex">\mu > 0</script></span> and degrees of freedom <span class="arithmatex"><span class="MathJax_Preview">\nu &gt; 0</span><script type="math/tex">\nu > 0</script></span>, denoted by <span class="arithmatex"><span class="MathJax_Preview">Y \sim \gamma(\mu, \nu)</span><script type="math/tex">Y \sim \gamma(\mu, \nu)</script></span>, if its p.d.f. is:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
f_\gamma(y | \mu, \nu) \equiv 
\begin{cases} 
    c_\gamma^{-1} y^{\frac{\nu}{2} - 1} \exp\left(-\frac{y \nu}{2\mu}\right), &amp; \text{if } 0 &lt; y &lt; \infty, \\
    0, &amp; \text{otherwise},
\end{cases}
</div>
<script type="math/tex; mode=display">
f_\gamma(y | \mu, \nu) \equiv 
\begin{cases} 
    c_\gamma^{-1} y^{\frac{\nu}{2} - 1} \exp\left(-\frac{y \nu}{2\mu}\right), & \text{if } 0 < y < \infty, \\
    0, & \text{otherwise},
\end{cases}
</script>
</div>
<p>where the integrating constant is given by:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
c_\gamma = \left(\frac{2\mu}{\nu}\right)^{\frac{\nu}{2}} \Gamma\left(\frac{\nu}{2}\right),
</div>
<script type="math/tex; mode=display">
c_\gamma = \left(\frac{2\mu}{\nu}\right)^{\frac{\nu}{2}} \Gamma\left(\frac{\nu}{2}\right),
</script>
</div>
<p>and <span class="arithmatex"><span class="MathJax_Preview">\Gamma(a)</span><script type="math/tex">\Gamma(a)</script></span> is the Gamma function (see Poirier, 1995, p. 98).</p>
<div class="highlight"><pre><span></span><code><span class="c1"># some dummy parameters</span>
<span class="n">nuprior</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">Vprior_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>
<span class="n">s_sqprior</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">beta_prior</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>

<span class="nd">@jit</span><span class="p">(</span><span class="n">nopython</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">posterior_params</span><span class="p">(</span><span class="n">beta_prior</span><span class="p">,</span> <span class="n">Vprior_inv</span><span class="p">,</span> <span class="n">s_sqprior</span><span class="p">,</span> <span class="n">nuprior</span><span class="p">,</span> <span class="n">beta_hat</span><span class="p">,</span> <span class="n">s_sq</span><span class="p">,</span> <span class="n">nu</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">XX</span><span class="p">):</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">XX</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">Vpost_inv</span> <span class="o">=</span> <span class="n">Vprior_inv</span> <span class="o">+</span> <span class="n">XX</span>
    <span class="n">beta_post</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">Vpost_inv</span><span class="p">,</span> <span class="n">Vprior_inv</span> <span class="o">@</span> <span class="n">beta_prior</span> <span class="o">+</span> <span class="n">XX</span> <span class="o">@</span> <span class="n">beta_hat</span><span class="p">)</span>
    <span class="n">nupost</span> <span class="o">=</span> <span class="n">nuprior</span> <span class="o">+</span> <span class="n">N</span>
    <span class="n">s_sqpost</span> <span class="o">=</span> <span class="p">(</span><span class="n">nuprior</span> <span class="o">*</span> <span class="n">s_sqprior</span> \
        <span class="o">+</span> <span class="n">nu</span> <span class="o">*</span> <span class="n">s_sq</span> \
        <span class="o">+</span> <span class="p">(</span><span class="n">beta_hat</span> <span class="o">-</span> <span class="n">beta_prior</span><span class="p">)</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">Vprior_inv</span> \
        <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">K</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">Vprior_inv</span> <span class="o">@</span> <span class="n">XX</span><span class="p">),</span> <span class="p">(</span><span class="n">beta_hat</span> <span class="o">-</span> <span class="n">beta_prior</span><span class="p">))</span>
        <span class="p">)</span> <span class="o">/</span> <span class="n">nupost</span>
    <span class="k">return</span> <span class="n">beta_post</span><span class="p">,</span><span class="n">Vpost_inv</span><span class="p">,</span> <span class="n">s_sqpost</span><span class="p">,</span> <span class="n">nupost</span>

<span class="n">betapost</span><span class="p">,</span> <span class="n">Vpost_inv</span><span class="p">,</span> <span class="n">s_sqpost</span><span class="p">,</span> <span class="n">nupost</span> <span class="o">=</span> <span class="n">posterior_params</span><span class="p">(</span><span class="n">beta_prior</span><span class="p">,</span> <span class="n">Vprior_inv</span><span class="p">,</span> <span class="n">s_sqprior</span><span class="p">,</span> <span class="n">nuprior</span><span class="p">,</span> <span class="n">beta_hat</span><span class="p">,</span> <span class="n">s_sq</span><span class="p">,</span> <span class="n">nu</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">XX</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="o">%%</span><span class="n">timeit</span>
<span class="n">posterior_params</span><span class="p">(</span><span class="n">beta_prior</span><span class="p">,</span> <span class="n">Vprior_inv</span><span class="p">,</span> <span class="n">s_sqprior</span><span class="p">,</span> <span class="n">nuprior</span><span class="p">,</span> <span class="n">beta_hat</span><span class="p">,</span> <span class="n">s_sq</span><span class="p">,</span> <span class="n">nu</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">XX</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>3.69 μs ± 345 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)
</code></pre></div>
<p><strong>The marginal posterior for <span class="arithmatex"><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span>:</strong> is a multivariate <span class="arithmatex"><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span>-distribution</p>
<div class="arithmatex">
<div class="MathJax_Preview">\beta | y \sim t\left(\overline{\beta}, \overline{s}^2 \overline{V}, \overline{\nu}\right).</div>
<script type="math/tex; mode=display">\beta | y \sim t\left(\overline{\beta}, \overline{s}^2 \overline{V}, \overline{\nu}\right).</script>
</div>
<p>For such holds </p>
<div class="arithmatex">
<div class="MathJax_Preview">E(\beta | y) = \overline{\beta}</div>
<script type="math/tex; mode=display">E(\beta | y) = \overline{\beta}</script>
</div>
<p>and</p>
<div class="arithmatex">
<div class="MathJax_Preview">\text{var}(\beta | y) = \frac{\overline{\nu} \overline{s}^2}{\overline{\nu} - 2} \overline{V}.</div>
<script type="math/tex; mode=display">\text{var}(\beta | y) = \frac{\overline{\nu} \overline{s}^2}{\overline{\nu} - 2} \overline{V}.</script>
</div>
<p>Intuition: Posterior mean and variance are a weighted average of information in the prior and the data.</p>
<p><strong>Task [marginal posterior]:</strong></p>
<p>Write a function which will compute the marginal posterior for <span class="arithmatex"><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span>.</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">multivariate_t</span>
<span class="k">def</span> <span class="nf">mposterior_beta</span><span class="p">(</span><span class="n">betapost</span><span class="p">,</span> <span class="n">Vpost_inv</span><span class="p">,</span> <span class="n">s_sq</span><span class="p">,</span> <span class="n">nupost</span><span class="p">):</span>
    <span class="n">Vpost</span> <span class="o">=</span>  <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">Vpost_inv</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">multivariate_t</span><span class="p">(</span><span class="n">betapost</span><span class="p">,</span> <span class="n">s_sq</span> <span class="o">*</span><span class="n">Vpost</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="n">nupost</span><span class="p">)</span>
    <span class="n">betapost_var</span> <span class="o">=</span> <span class="n">nupost</span> <span class="o">*</span> <span class="n">s_sq</span> <span class="o">*</span> <span class="n">Vpost</span><span class="o">/</span><span class="p">(</span><span class="n">nupost</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># variance of the t-distribution</span>
    <span class="k">return</span> <span class="n">betapost</span><span class="p">,</span> <span class="n">betapost_var</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">Vpost</span> <span class="c1"># mean, variance, t-distribution, covariance matrix of posterior</span>

<span class="c1"># check that it works</span>
<span class="n">mposterior_beta</span><span class="p">(</span><span class="n">betapost</span><span class="p">,</span> <span class="n">Vpost_inv</span><span class="p">,</span> <span class="n">s_sqpost</span><span class="p">,</span> <span class="n">nupost</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>(array([0.89285165, 2.12517763, 3.03015342]),
 array([[ 0.00671138, -0.00550244, -0.00587363],
        [-0.00550244,  0.01109588, -0.00023851],
        [-0.00587363, -0.00023851,  0.01206552]]),
 &lt;scipy.stats._multivariate.multivariate_t_frozen at 0x157873640&gt;,
 array([[ 0.00666605, -0.00546527, -0.00583395],
        [-0.00546527,  0.01102093, -0.0002369 ],
        [-0.00583395, -0.0002369 ,  0.01198402]]))
</code></pre></div>
<p>Finally, we wish to initialize the regression model with a diffuse prior: <span class="arithmatex"><span class="MathJax_Preview">\underline{V} = c I_K</span><script type="math/tex">\underline{V} = c I_K</script></span> with <span class="arithmatex"><span class="MathJax_Preview">c</span><script type="math/tex">c</script></span> large. We modularize our regression model to handle instances of it better.</p>
<p><strong>Task [<code>BayesianRegressionNC</code> class]:</strong></p>
<ol>
<li>
<p>Write a class <code>BayesianRegressionNC</code> which features methods <code>set_prior</code>, <code>get_posterior</code> and <code>get_mposterior</code> which set the prior, and calculate posterior and marginal posterior, respectively, for the natural conjugate. Make use of the <code>ols_solution</code> function whenever you can, and use <code>multivariate_t</code> from the <code>scipy</code> library.</p>
</li>
<li>
<p>Add a <code>predict</code> method which takes a matrix <code>X_new</code> as input.</p>
</li>
</ol>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">BayesianRegressionNC</span><span class="p">():</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the class with data</span>
<span class="sd">        y: the dependent variable</span>
<span class="sd">        X: the independent variables</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">y</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta_hat</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_sq</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nu</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">XX</span> <span class="o">=</span> <span class="n">ols_solution</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ols</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;beta&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta_hat</span><span class="p">,</span>
            <span class="s1">&#39;s_sq&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_sq</span><span class="p">,</span>
            <span class="s1">&#39;nu&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">nu</span>
        <span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prior</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;beta&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s1">&#39;nu&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s1">&#39;s_sq&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s1">&#39;V_inv&#39;</span><span class="p">:</span> <span class="kc">None</span>
        <span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">posterior</span> <span class="o">=</span> <span class="p">{</span> <span class="c1"># posterior for beta and h</span>
            <span class="s1">&#39;beta&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s1">&#39;V_inv&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s1">&#39;s_sq&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s1">&#39;nu&#39;</span><span class="p">:</span> <span class="kc">None</span>
        <span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mposterior</span> <span class="o">=</span> <span class="p">{</span> <span class="c1"># marginal posterior for beta</span>
            <span class="s1">&#39;beta&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s1">&#39;beta_var&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s1">&#39;t&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s1">&#39;V&#39;</span><span class="p">:</span> <span class="kc">None</span>
        <span class="p">}</span>
        <span class="c1">#self.betaprior, self.nuprior, self.s_sqprior, self.Vprior_inv = None, None, None, None</span>

    <span class="k">def</span> <span class="nf">set_prior</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">betaprior</span><span class="p">,</span> <span class="n">Vprior_inv</span><span class="p">,</span> <span class="n">s_sqprior</span><span class="p">,</span> <span class="n">nuprior</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Set the prior parameters&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prior</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;beta&#39;</span><span class="p">:</span> <span class="n">betaprior</span><span class="p">,</span>
            <span class="s1">&#39;nu&#39;</span><span class="p">:</span> <span class="n">nuprior</span><span class="p">,</span>
            <span class="s1">&#39;s_sq&#39;</span><span class="p">:</span> <span class="n">s_sqprior</span><span class="p">,</span>
            <span class="s1">&#39;V_inv&#39;</span><span class="p">:</span> <span class="n">Vprior_inv</span>
        <span class="p">}</span>

    <span class="k">def</span> <span class="nf">get_posterior</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Set the posterior parameters&quot;&quot;&quot;</span>
        <span class="n">betaprior</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">[</span><span class="s1">&#39;beta&#39;</span><span class="p">]</span>
        <span class="n">Vprior_inv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">[</span><span class="s1">&#39;V_inv&#39;</span><span class="p">]</span>
        <span class="n">s_sqprior</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">[</span><span class="s1">&#39;s_sq&#39;</span><span class="p">]</span>
        <span class="n">nuprior</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">[</span><span class="s1">&#39;nu&#39;</span><span class="p">]</span>

        <span class="n">betapost</span><span class="p">,</span> <span class="n">Vpost_inv</span><span class="p">,</span> <span class="n">s_sqpost</span><span class="p">,</span> <span class="n">nupost</span> <span class="o">=</span> <span class="n">posterior_params</span><span class="p">(</span><span class="n">betaprior</span><span class="p">,</span> <span class="n">Vprior_inv</span><span class="p">,</span> <span class="n">s_sqprior</span><span class="p">,</span> <span class="n">nuprior</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta_hat</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_sq</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nu</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">XX</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">posterior</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;beta&#39;</span><span class="p">:</span> <span class="n">betapost</span><span class="p">,</span>
            <span class="s1">&#39;V_inv&#39;</span><span class="p">:</span> <span class="n">Vpost_inv</span><span class="p">,</span>
            <span class="s1">&#39;s_sq&#39;</span><span class="p">:</span> <span class="n">s_sqpost</span><span class="p">,</span>
            <span class="s1">&#39;nu&#39;</span><span class="p">:</span> <span class="n">nupost</span>
        <span class="p">}</span>

    <span class="k">def</span> <span class="nf">get_mposterior</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Set the marginal posterior parameters&quot;&quot;&quot;</span>
        <span class="n">betapost</span><span class="p">,</span> <span class="n">Vpost_inv</span><span class="p">,</span> <span class="n">s_sqpost</span><span class="p">,</span> <span class="n">nupost</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">posterior</span><span class="p">[</span><span class="s1">&#39;beta&#39;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">posterior</span><span class="p">[</span><span class="s1">&#39;V_inv&#39;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">posterior</span><span class="p">[</span><span class="s1">&#39;s_sq&#39;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">posterior</span><span class="p">[</span><span class="s1">&#39;nu&#39;</span><span class="p">]</span>
        <span class="n">betapost</span><span class="p">,</span> <span class="n">betapost_var</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">Vpost</span> <span class="o">=</span> <span class="n">mposterior_beta</span><span class="p">(</span><span class="n">betapost</span><span class="p">,</span> <span class="n">Vpost_inv</span><span class="p">,</span> <span class="n">s_sqpost</span><span class="p">,</span> <span class="n">nupost</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mposterior</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;beta&#39;</span><span class="p">:</span> <span class="n">betapost</span><span class="p">,</span>
            <span class="s1">&#39;beta_var&#39;</span><span class="p">:</span> <span class="n">betapost_var</span><span class="p">,</span>
            <span class="s1">&#39;t&#39;</span><span class="p">:</span> <span class="n">t</span><span class="p">,</span>
            <span class="s1">&#39;V&#39;</span><span class="p">:</span> <span class="n">Vpost</span>
        <span class="p">}</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">Xnew</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Predict the dependent variable for new data using the posterior&quot;&quot;&quot;</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="n">Xnew</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">mposterior</span><span class="p">[</span><span class="s1">&#39;beta&#39;</span><span class="p">]</span>
        <span class="n">s_sq</span><span class="p">,</span> <span class="n">nu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">posterior</span><span class="p">[</span><span class="s1">&#39;s_sq&#39;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">posterior</span><span class="p">[</span><span class="s1">&#39;nu&#39;</span><span class="p">]</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">s_sq</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">Xnew</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">+</span> <span class="n">Xnew</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">posterior</span><span class="p">[</span><span class="s1">&#39;V_inv&#39;</span><span class="p">])</span> <span class="o">@</span> <span class="n">Xnew</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;y_hat&#39;</span><span class="p">:</span> <span class="n">mean</span><span class="p">,</span>
            <span class="s1">&#39;V&#39;</span><span class="p">:</span> <span class="n">V</span><span class="p">,</span>
            <span class="s1">&#39;t&#39;</span><span class="p">:</span> <span class="n">multivariate_t</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="n">nu</span><span class="p">)</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="n">predictions</span>
</code></pre></div>
<p>Let&rsquo;s try out the new class. Set a diffuse prior, and obtain posterior and marginal posterior. Play around with the code to ensure that when setting the diffuse prior, the posterior mean approaches the OLS solution.</p>
<div class="highlight"><pre><span></span><code><span class="c1">## initialize the class</span>
<span class="n">bmod</span> <span class="o">=</span> <span class="n">BayesianRegressionNC</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>

<span class="c1">## set the prior</span>
<span class="n">bmod</span><span class="o">.</span><span class="n">set_prior</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> 
               <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">*</span><span class="mf">1E-5</span><span class="p">,</span> 
               <span class="mi">1</span><span class="p">,</span> 
               <span class="mi">0</span><span class="p">)</span>

<span class="c1">## get the posterior given y and X</span>
<span class="n">bmod</span><span class="o">.</span><span class="n">get_posterior</span><span class="p">()</span>

<span class="c1">## get the marginal posterior for beta</span>
<span class="n">bmod</span><span class="o">.</span><span class="n">get_mposterior</span><span class="p">()</span>

<span class="c1">## check the results</span>
<span class="nb">print</span><span class="p">(</span><span class="n">bmod</span><span class="o">.</span><span class="n">posterior</span><span class="p">,</span> <span class="n">bmod</span><span class="o">.</span><span class="n">mposterior</span><span class="p">,</span> <span class="n">bmod</span><span class="o">.</span><span class="n">ols</span><span class="p">,</span> <span class="n">bmod</span><span class="o">.</span><span class="n">prior</span><span class="p">)</span>

<span class="c1">## prediction</span>
<span class="n">Xnew</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">bmod</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xnew</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>{&#39;beta&#39;: array([0.86907205, 2.14332394, 3.06126144]), &#39;V_inv&#39;: array([[1000.00001   ,  507.08542046,  497.32198302],
       [ 507.08542046,  346.65378599,  253.72723223],
       [ 497.32198302,  253.72723223,  329.56200832]]), &#39;s_sq&#39;: 0.9935805373327228, &#39;nu&#39;: 1000} {&#39;beta&#39;: array([0.86907205, 2.14332394, 3.06126144]), &#39;beta_var&#39;: array([[ 0.00674628, -0.00553757, -0.00591707],
       [-0.00553757,  0.01112504, -0.00020868],
       [-0.00591707, -0.00020868,  0.01211065]]), &#39;t&#39;: &lt;scipy.stats._multivariate.multivariate_t_frozen object at 0x4bde56230&gt;, &#39;V&#39;: array([[ 0.00677629, -0.0055622 , -0.00594339],
       [-0.0055622 ,  0.01117453, -0.00020961],
       [-0.00594339, -0.00020961,  0.01216452]])} {&#39;beta&#39;: array([0.86907181, 2.14332413, 3.06126176]), &#39;s_sq&#39;: 0.9965702467624876, &#39;nu&#39;: 997} {&#39;beta&#39;: array([0., 0., 0.]), &#39;nu&#39;: 0, &#39;s_sq&#39;: 1, &#39;V_inv&#39;: array([[1.e-05, 0.e+00, 0.e+00],
       [0.e+00, 1.e-05, 0.e+00],
       [0.e+00, 0.e+00, 1.e-05]])}
{&#39;y_hat&#39;: array([4.6031991 , 2.62950475]), &#39;V&#39;: array([[0.99743727, 0.00130722],
       [0.00130722, 0.99907623]]), &#39;t&#39;: &lt;scipy.stats._multivariate.multivariate_t_frozen object at 0x4bded7dc0&gt;}
</code></pre></div>
<h3 id="prediction-and-monte-carlo-integration"><strong>Prediction and Monte Carlo Integration</strong><a class="headerlink" href="#prediction-and-monte-carlo-integration" title="Permanent link">#</a></h3>
<p>We now wish to calculate the probability of the some event by Monte Carlo integration, e.g.</p>
<div class="arithmatex">
<div class="MathJax_Preview"> Pr(\hat{y}(X^*_1) \in A | X). </div>
<script type="math/tex; mode=display"> Pr(\hat{y}(X^*_1) \in A | X). </script>
</div>
<p>Consider for example the case where</p>
<div class="arithmatex">
<div class="MathJax_Preview"> A = [0.9\cdot \hat{y}(X^*_2), 1.5 \cdot \hat{y}(X^*_2)].</div>
<script type="math/tex; mode=display"> A = [0.9\cdot \hat{y}(X^*_2), 1.5 \cdot \hat{y}(X^*_2)].</script>
</div>
<p>We now write a quick Monte-Carlo method to simulate this probability from our predictive density.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># First, simulate some predictions from the model</span>
<span class="n">sims</span> <span class="o">=</span> <span class="n">predictions</span><span class="p">[</span><span class="s1">&#39;t&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="mi">100_000</span><span class="p">)</span>

<span class="c1"># Second, calculate a boolean vector that is True if the first element is in A</span>
<span class="nb">bool</span> <span class="o">=</span> <span class="p">(</span><span class="n">sims</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">sims</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mf">0.9</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">sims</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">sims</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mf">1.5</span><span class="p">)</span>

<span class="c1"># Third, calculate the proportion of True values</span>
<span class="n">p</span> <span class="o">=</span> <span class="nb">bool</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Prob(A):&quot;</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>Prob(A): 0.3087
</code></pre></div>
<p>To finish this section, also plot the simulations and mark the draws which fall into <span class="arithmatex"><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span>.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">n_plot</span> <span class="o">=</span> <span class="mi">10_000</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">sims</span><span class="p">[:</span><span class="n">n_plot</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">sims</span><span class="p">[:</span><span class="n">n_plot</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="nb">bool</span><span class="p">[:</span><span class="n">n_plot</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Simulated posterior predictive distribution&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\hat</span><span class="si">{y}</span><span class="s1">(X^*_1)$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\hat</span><span class="si">{y}</span><span class="s1">(X^*_2)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="../BayReg_files/BayReg_19_0.png" /></p>
<h2 id="independent-normal-gamma-priors-and-heteroskedastic-errors"><strong>Independent Normal-Gamma Priors and Heteroskedastic Errors</strong><a class="headerlink" href="#independent-normal-gamma-priors-and-heteroskedastic-errors" title="Permanent link">#</a></h2>
<h3 id="model-set-up"><strong>Model Set-Up</strong><a class="headerlink" href="#model-set-up" title="Permanent link">#</a></h3>
<p>Keeping the Normal linear regression model, we now assume independence of <span class="arithmatex"><span class="MathJax_Preview">\beta, h</span><script type="math/tex">\beta, h</script></span>. As a consequence, the posterior will not have a closed form solution anymore, and we will need to write a routine - the Gibbs sampler - which allows us to draw samples from the posterior. We also include heteroskedasticity by intrducing a random diagonal matrix, <span class="arithmatex"><span class="MathJax_Preview">\Omega</span><script type="math/tex">\Omega</script></span>. That is,</p>
<div class="arithmatex">
<div class="MathJax_Preview">
    \epsilon \sim N(0_N, h^{-1}\Omega)
</div>
<script type="math/tex; mode=display">
    \epsilon \sim N(0_N, h^{-1}\Omega)
</script>
</div>
<p>Now assume that </p>
<div class="arithmatex">
<div class="MathJax_Preview">
    p(\beta, h, \Omega) = p(\beta)p(h)p(\Omega)
</div>
<script type="math/tex; mode=display">
    p(\beta, h, \Omega) = p(\beta)p(h)p(\Omega)
</script>
</div>
<p>with</p>
<div class="arithmatex">
<div class="MathJax_Preview">
    \beta \sim N(\underline \beta, \underline V), \\
    h \sim G(\underline s^2, \underline \nu), \\
    \Omega = diag(\lambda_1^{-1}, ..., \lambda_N^{-1}), \\
    p(\lambda) = \prod_i f_G(\lambda_i | 1, \nu_\lambda),
</div>
<script type="math/tex; mode=display">
    \beta \sim N(\underline \beta, \underline V), \\
    h \sim G(\underline s^2, \underline \nu), \\
    \Omega = diag(\lambda_1^{-1}, ..., \lambda_N^{-1}), \\
    p(\lambda) = \prod_i f_G(\lambda_i | 1, \nu_\lambda),
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">f_G</span><script type="math/tex">f_G</script></span> is the Gamma pdf. The difference to the natural conjugate prior is that <span class="arithmatex"><span class="MathJax_Preview">\underline V</span><script type="math/tex">\underline V</script></span> is now the prior covariance of <span class="arithmatex"><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span>, independent of <span class="arithmatex"><span class="MathJax_Preview">h</span><script type="math/tex">h</script></span>.</p>
<p><strong>Parameter-Posteriors</strong></p>
<p>The joint posterior of <span class="arithmatex"><span class="MathJax_Preview">\beta, h</span><script type="math/tex">\beta, h</script></span> does not take form of a nice, closed form density. However, the conditional posterior for <span class="arithmatex"><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span> can be written as</p>
<div class="arithmatex">
<div class="MathJax_Preview">
    \beta | y, h, \Omega \sim N(\overline \beta,\overline V), 
</div>
<script type="math/tex; mode=display">
    \beta | y, h, \Omega \sim N(\overline \beta,\overline V), 
</script>
</div>
<p>where</p>
<div class="arithmatex">
<div class="MathJax_Preview">
    \overline V = \big( \underline V^{-1} + h X'\Omega^{-1}X \big)^{-1}, \\
    \overline \beta  = \overline V \big( \underline V^{-1} \underline\beta + h X'\Omega^{-1}X\hat{\beta}(\Omega) \big).
</div>
<script type="math/tex; mode=display">
    \overline V = \big( \underline V^{-1} + h X'\Omega^{-1}X \big)^{-1}, \\
    \overline \beta  = \overline V \big( \underline V^{-1} \underline\beta + h X'\Omega^{-1}X\hat{\beta}(\Omega) \big).
</script>
</div>
<p><span class="arithmatex"><span class="MathJax_Preview">\hat{\beta}(\Omega)</span><script type="math/tex">\hat{\beta}(\Omega)</script></span> is the GLS estimator. The conditional posterior for <span class="arithmatex"><span class="MathJax_Preview">h</span><script type="math/tex">h</script></span> takes a simple form, too:  </p>
<div class="arithmatex">
<div class="MathJax_Preview">
    h | y, \beta, \Omega \sim G\left(\overline{s}^{-2}, \overline{\nu}\right)
</div>
<script type="math/tex; mode=display">
    h | y, \beta, \Omega \sim G\left(\overline{s}^{-2}, \overline{\nu}\right)
</script>
</div>
<p>where:  </p>
<div class="arithmatex">
<div class="MathJax_Preview">
    \overline{\nu} = N + \underline\nu
</div>
<script type="math/tex; mode=display">
    \overline{\nu} = N + \underline\nu
</script>
</div>
<p>and:  </p>
<div class="arithmatex">
<div class="MathJax_Preview">
    \overline{s}^2 = \frac{(y - X\beta)'(y - X\beta) + \underline\nu \underline s^2}{\overline{\nu}}
</div>
<script type="math/tex; mode=display">
    \overline{s}^2 = \frac{(y - X\beta)'(y - X\beta) + \underline\nu \underline s^2}{\overline{\nu}}
</script>
</div>
<p>Generally, the conditional posterior for <span class="arithmatex"><span class="MathJax_Preview">\Omega</span><script type="math/tex">\Omega</script></span> is given by</p>
<div class="arithmatex">
<div class="MathJax_Preview">
    p(\Omega | y,\beta, h) \propto p(\Omega) |\Omega|^{-\frac{1}{2}} \exp\big( - \frac{h}{2} (y - X\beta)'\Omega^{-1}(y - X\beta) \big).
</div>
<script type="math/tex; mode=display">
    p(\Omega | y,\beta, h) \propto p(\Omega) |\Omega|^{-\frac{1}{2}} \exp\big( - \frac{h}{2} (y - X\beta)'\Omega^{-1}(y - X\beta) \big).
</script>
</div>
<p>In the model with Gamma distributed <span class="arithmatex"><span class="MathJax_Preview">\lambda_i</span><script type="math/tex">\lambda_i</script></span>, one obtains </p>
<div class="arithmatex">
<div class="MathJax_Preview">
    p(\lambda_i | y, h, \beta, \nu_\lambda) = f_G\bigg( \lambda_i \big| \frac{\nu_\lambda + 1}{h \epsilon_i^2 + \nu_\lambda}, \nu_\lambda +1  \bigg)
</div>
<script type="math/tex; mode=display">
    p(\lambda_i | y, h, \beta, \nu_\lambda) = f_G\bigg( \lambda_i \big| \frac{\nu_\lambda + 1}{h \epsilon_i^2 + \nu_\lambda}, \nu_\lambda +1  \bigg)
</script>
</div>
<p>The conditional posteriors do not directly tell us about <span class="arithmatex"><span class="MathJax_Preview">p(\beta, h, \Omega | y)</span><script type="math/tex">p(\beta, h, \Omega | y)</script></span>. Because we are interested in <span class="arithmatex"><span class="MathJax_Preview">p(\beta, h | y)</span><script type="math/tex">p(\beta, h | y)</script></span> (or <span class="arithmatex"><span class="MathJax_Preview">p(\beta | y)</span><script type="math/tex">p(\beta | y)</script></span> and <span class="arithmatex"><span class="MathJax_Preview">p(h | y)</span><script type="math/tex">p(h | y)</script></span>), and not the posterior conditionals, we use a posterior simulator, called the <strong>Gibbs sampler</strong>, which uses conditional posteriors to produce random draws, <span class="arithmatex"><span class="MathJax_Preview">\beta^{(s)}</span><script type="math/tex">\beta^{(s)}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">h^{(s)}</span><script type="math/tex">h^{(s)}</script></span> for <span class="arithmatex"><span class="MathJax_Preview">s = 1, \dots, S</span><script type="math/tex">s = 1, \dots, S</script></span>. (For a formal introduction to the Gibbs sampler, the <a href="https://en.wikipedia.org/wiki/Gibbs_sampling">Wikipedia article</a> is helpful.) These can be averaged to produce estimates of posterior properties just as with Monte Carlo integration.</p>
<h3 id="hyperparameters-and-m-h-random-walk"><strong>Hyperparameters and M-H Random Walk</strong><a class="headerlink" href="#hyperparameters-and-m-h-random-walk" title="Permanent link">#</a></h3>
<p>The parameter <span class="arithmatex"><span class="MathJax_Preview">\nu_\lambda</span><script type="math/tex">\nu_\lambda</script></span> is a hyperparameter, to which we assign a prior, too. One can show that if it follows an exponential prior, then its posterior density is, up to a constant factor: </p>
<div class="arithmatex">
<div class="MathJax_Preview">
    p(\nu_\lambda | y, \beta, h, \lambda) \propto \left(\frac{\nu_\lambda}{2}\right)^{\frac{N \nu_\lambda}{2}} \Gamma\left(\frac{\nu_\lambda}{2}\right)^{-N} \exp(-\eta \nu_\lambda),
</div>
<script type="math/tex; mode=display">
    p(\nu_\lambda | y, \beta, h, \lambda) \propto \left(\frac{\nu_\lambda}{2}\right)^{\frac{N \nu_\lambda}{2}} \Gamma\left(\frac{\nu_\lambda}{2}\right)^{-N} \exp(-\eta \nu_\lambda),
</script>
</div>
<p>where</p>
<div class="arithmatex">
<div class="MathJax_Preview">
    \eta = \frac{1}{\underline \nu_\lambda} + 0.5 \sum_{i=1}^{N} (-\ln(\lambda_i) + \lambda_i).
</div>
<script type="math/tex; mode=display">
    \eta = \frac{1}{\underline \nu_\lambda} + 0.5 \sum_{i=1}^{N} (-\ln(\lambda_i) + \lambda_i).
</script>
</div>
<p>The obvious question is: How can we sample from <span class="arithmatex"><span class="MathJax_Preview">p(\nu_\lambda | y, \beta, h, \lambda)</span><script type="math/tex">p(\nu_\lambda | y, \beta, h, \lambda)</script></span>? To do so, we us a <strong>Metropolis-Hastings Random Walk algorithm</strong> (MHRW). The Metropolis-Hastings random walk algorithm is a Markov Chain Monte Carlo (MCMC) method used to generate samples from a probability distribution when direct sampling is difficult. In general terms, the algorithm aims to sample from a target distribution $ \pi(x) $ when its normalization constant is unknown or intractable.</p>
<p><strong>Algorithm Steps</strong></p>
<ol>
<li>
<p><strong>Initialize:</strong> Start at an initial state <span class="arithmatex"><span class="MathJax_Preview">x_0</span><script type="math/tex">x_0</script></span>.</p>
</li>
<li>
<p><strong>Proposal Step:</strong> Generate a proposed new state <span class="arithmatex"><span class="MathJax_Preview">x'</span><script type="math/tex">x'</script></span> using a <em>proposal distribution</em> <span class="arithmatex"><span class="MathJax_Preview">q(x' | x)</span><script type="math/tex">q(x' | x)</script></span>, often symmetric (e.g., a normal distribution centered at the current state).<br />
   $$
   x&rsquo; = x + \epsilon \quad \text{where } \epsilon \sim N(0, \sigma^2)
   $$</p>
</li>
<li>
<p><strong>Acceptance Probability:</strong> Compute the acceptance ratio:<br />
   $$
   \alpha = \min\left(1, \frac{\pi(x&rsquo;) \cdot q(x | x&rsquo;)}{\pi(x) \cdot q(x&rsquo; | x)}\right)
   $$  </p>
</li>
</ol>
<p>For symmetric proposal distributions where <span class="arithmatex"><span class="MathJax_Preview">q(x' | x) = q(x | x')</span><script type="math/tex">q(x' | x) = q(x | x')</script></span>, this simplifies to:  </p>
<p>$$
   \alpha = \min\left(1, \frac{\pi(x&rsquo;)}{\pi(x)}\right)
   $$</p>
<ol>
<li><strong>Accept/Reject:</strong> Generate a uniform random number <span class="arithmatex"><span class="MathJax_Preview">u \sim U(0, 1)</span><script type="math/tex">u \sim U(0, 1)</script></span>.  </li>
<li>If <span class="arithmatex"><span class="MathJax_Preview">u \leq \alpha</span><script type="math/tex">u \leq \alpha</script></span>, accept the proposal and set <span class="arithmatex"><span class="MathJax_Preview">x_{t+1} = x'</span><script type="math/tex">x_{t+1} = x'</script></span>.</li>
<li>
<p>Otherwise, reject the proposal and set <span class="arithmatex"><span class="MathJax_Preview">x_{t+1} = x</span><script type="math/tex">x_{t+1} = x</script></span>.</p>
</li>
<li>
<p><strong>Repeat:</strong> Continue the process for <span class="arithmatex"><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span> iterations to build a Markov chain of samples.</p>
</li>
</ol>
<p>The Markov chain eventually converges to the target distribution <span class="arithmatex"><span class="MathJax_Preview">\pi(x)</span><script type="math/tex">\pi(x)</script></span>. The random walk proposal (e.g., normal step) ensures local exploration. Choosing the step size (via the proposal variance <span class="arithmatex"><span class="MathJax_Preview">\sigma^2</span><script type="math/tex">\sigma^2</script></span>) balances mixing and efficiency.</p>
<p><strong>Task [Metropolis-Hastings]:</strong>
Write and illustrate the MHRW algorithm by sampling from a double-exponential (Laplace) distribution.</p>
<p><strong>Solution:</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Define the target distribution: Double Exponential (Laplace) Distribution</span>
<span class="k">def</span> <span class="nf">target_density</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="c1"># Double exponential (Laplace) PDF with mean=0, b=1</span>
    <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="nb">abs</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="c1"># Metropolis-Hastings Random Walk Algorithm</span>
<span class="k">def</span> <span class="nf">metropolis_hastings</span><span class="p">(</span><span class="n">target_density</span><span class="p">,</span> <span class="n">proposal_std</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">,</span> <span class="n">initial_state</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="p">[</span><span class="n">initial_state</span><span class="p">]</span>
    <span class="n">current_state</span> <span class="o">=</span> <span class="n">initial_state</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_samples</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="c1"># Step 1: Propose a new state using a normal distribution centered at current state</span>
        <span class="n">proposed_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">current_state</span><span class="p">,</span> <span class="n">proposal_std</span><span class="p">)</span>

        <span class="c1"># Step 2: Calculate acceptance probability</span>
        <span class="n">acceptance_ratio</span> <span class="o">=</span> <span class="n">target_density</span><span class="p">(</span><span class="n">proposed_state</span><span class="p">)</span> <span class="o">/</span> <span class="n">target_density</span><span class="p">(</span><span class="n">current_state</span><span class="p">)</span>
        <span class="n">acceptance_prob</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">acceptance_ratio</span><span class="p">)</span>

        <span class="c1"># Step 3: Accept or reject the proposal</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">acceptance_prob</span><span class="p">:</span>
            <span class="n">current_state</span> <span class="o">=</span> <span class="n">proposed_state</span>  <span class="c1"># Accept proposal</span>

        <span class="c1"># Step 4: Store the current state</span>
        <span class="n">samples</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_state</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>

<span class="c1"># Parameters</span>
<span class="n">proposal_std</span> <span class="o">=</span> <span class="mf">1.0</span>   <span class="c1"># Standard deviation of the proposal distribution</span>
<span class="n">num_samples</span> <span class="o">=</span> <span class="mi">10000</span>  <span class="c1"># Number of samples to generate</span>

<span class="c1"># Run the MHRW sampling</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">metropolis_hastings</span><span class="p">(</span><span class="n">target_density</span><span class="p">,</span> <span class="n">proposal_std</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">)</span>

<span class="c1"># Plot the histogram of samples</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;skyblue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;MHRW Samples&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">target_density</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;True Double-Exponential PDF&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Density&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;MHRW Sampling from Double-Exponential Distribution&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="../BayReg_files/BayReg_23_0.png" /></p>
<p>We see that the MHRW sampler yields correlated samples between sample <span class="arithmatex"><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> and <span class="arithmatex"><span class="MathJax_Preview">i+1</span><script type="math/tex">i+1</script></span>. Note that <span class="arithmatex"><span class="MathJax_Preview">x_{i+1} | x_i</span><script type="math/tex">x_{i+1} | x_i</script></span> is <em>not</em> Laplace (<span class="arithmatex"><span class="MathJax_Preview">\pi(x)</span><script type="math/tex">\pi(x)</script></span>) distributed. <em>Only</em> the unconditional distribution, i.e. the histogram of <span class="arithmatex"><span class="MathJax_Preview">\{x_1, ..., x_N\}</span><script type="math/tex">\{x_1, ..., x_N\}</script></span> approximately resembles the density <span class="arithmatex"><span class="MathJax_Preview">\pi(x)</span><script type="math/tex">\pi(x)</script></span>. We can see the correlation by plotting the time path of samples. Because of its presence, the MHRW method may require sample replications.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Plot the time evolution of samples</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">sample_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">samples</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_samples</span><span class="o">-</span><span class="mi">100</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">),</span> <span class="n">samples</span><span class="p">[</span><span class="o">-</span><span class="mi">100</span><span class="p">:],</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Sample Path&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">markerfacecolor</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Sample ID&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Sample Value $x$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Time Evolution of MHRW Samples (last 100 samples)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="../BayReg_files/BayReg_25_0.png" /></p>
<h3 id="coding-the-full-model"><strong>Coding the Full Model</strong><a class="headerlink" href="#coding-the-full-model" title="Permanent link">#</a></h3>
<p>Let&rsquo;s code the model through a series of tasks.</p>
<p><strong>Task [GLS estimator]:</strong></p>
<p>Write a GLS estimator <span class="arithmatex"><span class="MathJax_Preview">\hat{\beta}(\Omega)</span><script type="math/tex">\hat{\beta}(\Omega)</script></span>, which takes the heteroskedasticity matrix <span class="arithmatex"><span class="MathJax_Preview">\Omega</span><script type="math/tex">\Omega</script></span> as given. Recall that <span class="arithmatex"><span class="MathJax_Preview">\Omega</span><script type="math/tex">\Omega</script></span> is diagonal.</p>
<p><strong>Task [posterior parameters]:</strong></p>
<p>Write a (set of) functions, which compute the conditional posterior parameters for <span class="arithmatex"><span class="MathJax_Preview">\beta, h, \Omega</span><script type="math/tex">\beta, h, \Omega</script></span>.</p>
<div class="highlight"><pre><span></span><code><span class="nd">@jit</span><span class="p">(</span><span class="n">nopython</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">gls_solution</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">Omega</span><span class="p">):</span>
    <span class="c1"># supply only the diagonal of Omega to avoid costly computation</span>
    <span class="n">P</span> <span class="o">=</span> <span class="n">Omega</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">Xg</span> <span class="o">=</span> <span class="n">X</span> <span class="o">*</span> <span class="n">P</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
    <span class="n">yg</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">P</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">XX</span> <span class="o">=</span> <span class="n">Xg</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">Xg</span>
    <span class="n">nu</span> <span class="o">=</span> <span class="n">N</span> <span class="o">-</span> <span class="n">K</span>
    <span class="n">beta_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">XX</span><span class="p">,</span> <span class="n">Xg</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">yg</span><span class="p">)</span>
    <span class="n">s_sq</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">nu</span> <span class="o">*</span> <span class="p">(</span><span class="n">yg</span> <span class="o">-</span> <span class="n">Xg</span> <span class="o">@</span> <span class="n">beta_hat</span><span class="p">)</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="p">(</span><span class="n">yg</span> <span class="o">-</span> <span class="n">Xg</span> <span class="o">@</span> <span class="n">beta_hat</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">beta_hat</span><span class="p">,</span> <span class="n">s_sq</span><span class="p">,</span> <span class="n">nu</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">XX</span>

<span class="k">def</span> <span class="nf">gls_dict_wrapper</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">Omega</span><span class="p">):</span>
    <span class="n">beta_hat</span><span class="p">,</span> <span class="n">s_sq</span><span class="p">,</span> <span class="n">nu</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">XOmega_invX</span> <span class="o">=</span> <span class="n">gls_solution</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">Omega</span><span class="p">)</span>
    <span class="n">gls_dict</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;beta&#39;</span><span class="p">:</span> <span class="n">beta_hat</span><span class="p">,</span>
        <span class="s1">&#39;s_sq&#39;</span><span class="p">:</span> <span class="n">s_sq</span><span class="p">,</span>
        <span class="s1">&#39;nu&#39;</span><span class="p">:</span> <span class="n">nu</span><span class="p">,</span>
        <span class="s1">&#39;N&#39;</span><span class="p">:</span> <span class="n">N</span><span class="p">,</span>
        <span class="s1">&#39;K&#39;</span><span class="p">:</span> <span class="n">K</span><span class="p">,</span>
        <span class="s1">&#39;XOmega_invX&#39;</span><span class="p">:</span> <span class="n">XOmega_invX</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">gls_dict</span>


<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;OLS estimator&#39;</span><span class="si">:</span><span class="s2">&lt;40</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">ols_solution</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">Omega</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># stable variance</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;GLS estimator, stable variance&#39;</span><span class="si">:</span><span class="s2">&lt;40</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">gls_solution</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">Omega</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">Omega</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># increasing variance</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;GLS estimator, increasing variance&#39;</span><span class="si">:</span><span class="s2">&lt;40</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">gls_solution</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">Omega</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>OLS estimator                            [0.86907181 2.14332413 3.06126176]
GLS estimator, stable variance           [0.86907181 2.14332413 3.06126176]
GLS estimator, increasing variance       [0.94638503 2.29210031 2.59479426]
</code></pre></div>
<p><strong>Task [Gibbs sampler (a)]</strong> Code up functions (e.g., <code>sample_beta</code>, <code>sample_P</code> and <code>sample_h</code>) to sample <span class="arithmatex"><span class="MathJax_Preview">\beta, \Omega</span><script type="math/tex">\beta, \Omega</script></span> and <span class="arithmatex"><span class="MathJax_Preview">h</span><script type="math/tex">h</script></span> from their conditional posterior distributions. </p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">scipy.special</span> <span class="kn">import</span> <span class="n">gammaln</span>

<span class="nd">@jit</span><span class="p">(</span><span class="n">nopython</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">numba_sum</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="nb">sum</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)):</span>
        <span class="nb">sum</span> <span class="o">+=</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="k">return</span> <span class="nb">sum</span>

<span class="c1">## SAMPLING BETAS</span>
<span class="c1"># beta given y, h, Omega</span>
<span class="nd">@jit</span><span class="p">(</span><span class="n">nopython</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">cond_post_param_beta</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">P_root</span><span class="p">,</span> <span class="n">beta_pri</span><span class="p">,</span> <span class="n">V_inv_pri</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="c1"># precision matrix, P =&gt; P_root^2</span>
    <span class="n">Xg</span> <span class="o">=</span> <span class="n">X</span> <span class="o">*</span> <span class="n">P_root</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
    <span class="n">yg</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">P_root</span>
    <span class="n">V_inv_pst</span> <span class="o">=</span> <span class="n">V_inv_pri</span> <span class="o">+</span> <span class="n">h</span> <span class="o">*</span> <span class="n">Xg</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">Xg</span>
    <span class="n">rhs</span> <span class="o">=</span> <span class="n">V_inv_pri</span> <span class="o">@</span> <span class="n">beta_pri</span> <span class="o">+</span> <span class="n">h</span> <span class="o">*</span> <span class="n">Xg</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">yg</span>
    <span class="n">beta_pst</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">V_inv_pst</span><span class="p">,</span> <span class="n">rhs</span><span class="p">)</span>  <span class="c1"># Solve V_inv_out @ x = rhs</span>
    <span class="k">return</span> <span class="n">beta_pst</span><span class="p">,</span> <span class="n">V_inv_pst</span> <span class="c1"># return posteriors for beta</span>

<span class="c1"># sample from the conditional posterior for beta</span>
<span class="k">def</span> <span class="nf">sample_beta</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">P_root</span><span class="p">,</span> <span class="n">beta_pri</span><span class="p">,</span> <span class="n">V_inv_pri</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">beta_pst</span><span class="p">,</span> <span class="n">V_inv_pst</span> <span class="o">=</span> <span class="n">cond_post_param_beta</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">P_root</span><span class="p">,</span> <span class="n">beta_pri</span><span class="p">,</span> <span class="n">V_inv_pri</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">beta_pst</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">V_inv_pst</span><span class="p">))</span>

<span class="nd">@jit</span><span class="p">(</span><span class="n">nopython</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">get_res</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">y</span> <span class="o">-</span> <span class="n">X</span> <span class="o">@</span> <span class="n">beta</span>

<span class="c1">## SAMPLING H</span>
<span class="nd">@jit</span><span class="p">(</span><span class="n">nopython</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">cond_post_param_h</span><span class="p">(</span><span class="n">P_root</span><span class="p">,</span> <span class="n">resid</span><span class="p">,</span> <span class="n">nu_pri</span><span class="p">,</span> <span class="n">s_sq_pri</span><span class="p">):</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">P_root</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">nu_pst</span> <span class="o">=</span> <span class="n">nu_pri</span> <span class="o">+</span> <span class="n">N</span>
    <span class="n">residg</span> <span class="o">=</span> <span class="n">resid</span> <span class="o">*</span> <span class="n">P_root</span> <span class="c1"># transform the residuals</span>
    <span class="n">s_sq_pst</span> <span class="o">=</span> <span class="n">residg</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">residg</span> 
    <span class="n">s_sq_pst</span> <span class="o">=</span> <span class="n">s_sq_pst</span> <span class="o">+</span> <span class="n">nu_pri</span> <span class="o">*</span> <span class="n">s_sq_pri</span>
    <span class="n">s_sq_pst</span> <span class="o">=</span> <span class="n">s_sq_pst</span> <span class="o">/</span> <span class="n">nu_pst</span>
    <span class="k">return</span> <span class="n">s_sq_pst</span><span class="p">,</span> <span class="n">nu_pst</span>

<span class="k">def</span> <span class="nf">sample_h</span><span class="p">(</span><span class="n">P_root</span><span class="p">,</span> <span class="n">resid</span><span class="p">,</span> <span class="n">nu_pri</span><span class="p">,</span> <span class="n">s_sq_pri</span><span class="p">):</span>
    <span class="n">s_sq_pst</span><span class="p">,</span> <span class="n">nu_pst</span> <span class="o">=</span> <span class="n">cond_post_param_h</span><span class="p">(</span><span class="n">P_root</span><span class="p">,</span> <span class="n">resid</span><span class="p">,</span> <span class="n">nu_pri</span><span class="p">,</span> <span class="n">s_sq_pri</span><span class="p">)</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">nu_pst</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">s_sq_pst</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">nu_pst</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">gamma</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">)</span>

<span class="c1">## SAMPLING P</span>
<span class="nd">@jit</span><span class="p">(</span><span class="n">nopython</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">cond_post_param_P</span><span class="p">(</span><span class="n">nu_lambda</span><span class="p">,</span> <span class="n">resid</span><span class="p">,</span> <span class="n">h</span><span class="p">):</span> 
    <span class="n">dof_lambda</span> <span class="o">=</span> <span class="n">nu_lambda</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">mu_lambda</span> <span class="o">=</span> <span class="p">(</span><span class="n">nu_lambda</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">h</span> <span class="o">*</span> <span class="n">resid</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">nu_lambda</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mu_lambda</span><span class="p">,</span> <span class="n">dof_lambda</span> 

<span class="k">def</span> <span class="nf">sample_P</span><span class="p">(</span><span class="n">resid</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">nu_lambda</span><span class="p">):</span>
    <span class="n">mu_lambda</span><span class="p">,</span> <span class="n">dof_lambda</span> <span class="o">=</span> <span class="n">cond_post_param_P</span><span class="p">(</span><span class="n">nu_lambda</span><span class="p">,</span> <span class="n">resid</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">dof_lambda</span><span class="o">/</span><span class="mi">2</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">mu_lambda</span> <span class="o">/</span> <span class="n">dof_lambda</span> <span class="c1"># this is a vector</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">gamma</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span> <span class="c1"># returns [... λ_i ...]</span>
</code></pre></div>
<p><strong>Task [Gibbs sampler (b)]</strong> Code up a function <code>sample_nu_lambda_new</code> to sample the next <span class="arithmatex"><span class="MathJax_Preview">\nu_\lambda</span><script type="math/tex">\nu_\lambda</script></span> from its conditional distribution. It is not necessary to fully nest the MHRW algorithm within the Gibbs sampler. Instead, the Gibbs sampler will run exactly <em>one step</em> of the Metropolis Hastings algorithm, hence <code>sample_nu_lambda_new</code> must take in the <span class="arithmatex"><span class="MathJax_Preview">\nu_\lambda</span><script type="math/tex">\nu_\lambda</script></span> from the previous Gibbs sampling iteration.</p>
<div class="highlight"><pre><span></span><code><span class="c1">## SAMPLING NU_LAMBDA</span>
<span class="nd">@jit</span><span class="p">(</span><span class="n">nopython</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">cond_post_param_nu_lambda</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">nu_lambda_pri</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute the parameter eta for the density of the posterior of nu_lambda</span>
<span class="sd">        given the current state of P (np.array)</span>

<span class="sd">        P = [λ1, ..., λN]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">lx</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">P</span><span class="p">)</span> <span class="o">+</span> <span class="n">P</span>
    <span class="n">eta</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">nu_lambda_pri</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">numba_sum</span><span class="p">(</span><span class="n">lx</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">eta</span>

<span class="k">def</span> <span class="nf">dof_density_prop_log</span><span class="p">(</span><span class="n">nu</span><span class="p">,</span> <span class="n">eta</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">nu</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">N</span> <span class="o">*</span> <span class="n">nu</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">gammaln</span><span class="p">(</span><span class="n">nu</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="o">-</span><span class="n">N</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="o">-</span><span class="n">eta</span> <span class="o">*</span> <span class="n">nu</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span>

<span class="k">def</span> <span class="nf">dof_density_ratio</span><span class="p">(</span><span class="n">nu_lambda_new</span><span class="p">,</span> <span class="n">nu_lambda_old</span><span class="p">,</span> <span class="n">eta</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">dof_density_prop_log</span><span class="p">(</span><span class="n">nu_lambda_new</span><span class="p">,</span> <span class="n">eta</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span> <span class="o">-</span> <span class="n">dof_density_prop_log</span><span class="p">(</span><span class="n">nu_lambda_old</span><span class="p">,</span> <span class="n">eta</span><span class="p">,</span> <span class="n">N</span><span class="p">))</span>

<span class="c1"># instead of the acceptance sampling, we use the random walk MH.</span>
<span class="k">def</span> <span class="nf">sample_nu_lambda_new</span><span class="p">(</span><span class="n">nu_lambda_old</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">nu_lambda_pri</span><span class="p">,</span> <span class="n">MH_param</span><span class="o">=</span><span class="mf">1.</span><span class="p">):</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># number of obs = number of latent lambdas</span>

    <span class="c1">## Use M-H step to draw new value for nu_lambda vector</span>
    <span class="c1"># 1. Propose a new value</span>
    <span class="n">nu_lambda_new</span> <span class="o">=</span> <span class="n">nu_lambda_old</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="n">MH_param</span><span class="p">)</span> <span class="c1"># random walk MH  </span>

    <span class="c1"># 2. Calculate the ratio of densities</span>
    <span class="k">if</span> <span class="n">nu_lambda_new</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="p">:</span> <span class="c1"># ensure it is positive</span>
        <span class="n">eta</span> <span class="o">=</span> <span class="n">cond_post_param_nu_lambda</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">nu_lambda_pri</span><span class="p">)</span> <span class="c1"># parameter in the density</span>
        <span class="n">acc_ratio</span> <span class="o">=</span> <span class="n">dof_density_ratio</span><span class="p">(</span><span class="n">nu_lambda_new</span><span class="p">,</span> <span class="n">nu_lambda_old</span><span class="p">,</span> <span class="n">eta</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span> <span class="c1">###  </span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">acc_ratio</span> <span class="o">=</span> <span class="mf">0.</span>

    <span class="c1"># 3. Accept or reject the new values</span>
    <span class="k">if</span> <span class="n">acc_ratio</span> <span class="o">&gt;=</span> <span class="mf">1.</span><span class="p">:</span>
        <span class="n">accept</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">elif</span> <span class="n">acc_ratio</span> <span class="o">==</span> <span class="mf">0.</span><span class="p">:</span>
        <span class="n">accept</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">accept</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">acc_ratio</span>

    <span class="c1"># 4. Return the new values, accepted or not</span>
    <span class="k">return</span> <span class="n">accept</span> <span class="o">*</span> <span class="n">nu_lambda_new</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">accept</span><span class="p">)</span> <span class="o">*</span> <span class="n">nu_lambda_old</span><span class="p">,</span> <span class="n">accept</span>
</code></pre></div>
<h3 id="gibbs-sampler">Gibbs sampler<a class="headerlink" href="#gibbs-sampler" title="Permanent link">#</a></h3>
<p>We now assemble the Gibbs sampler. The algorithm can be spelled out as follows:</p>
<p>Initialize: <span class="arithmatex"><span class="MathJax_Preview">\beta^{(0)} = \beta^{OLS}, h^{(0)} = \hat{\sigma}^2, \Omega^{(0)} = diag(\lambda_1^{-1}, ..., \lambda_N^{-1}) = diag(1/\boldsymbol{\lambda}^{(0)}), \boldsymbol{\lambda}^{(0)} = 1</span><script type="math/tex">\beta^{(0)} = \beta^{OLS}, h^{(0)} = \hat{\sigma}^2, \Omega^{(0)} = diag(\lambda_1^{-1}, ..., \lambda_N^{-1}) = diag(1/\boldsymbol{\lambda}^{(0)}), \boldsymbol{\lambda}^{(0)} = 1</script></span>.</p>
<ol>
<li><strong>Initialize:</strong></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\beta^{(0)} = \beta^{OLS}</span><script type="math/tex">\beta^{(0)} = \beta^{OLS}</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">h^{(0)} = \hat{\sigma}^2</span><script type="math/tex">h^{(0)} = \hat{\sigma}^2</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\Omega^{(0)} = \operatorname{diag}(\lambda_1^{-1}, \dots, \lambda_N^{-1}) = \operatorname{diag}(1/\boldsymbol{\lambda}^{(0)})</span><script type="math/tex">\Omega^{(0)} = \operatorname{diag}(\lambda_1^{-1}, \dots, \lambda_N^{-1}) = \operatorname{diag}(1/\boldsymbol{\lambda}^{(0)})</script></span></li>
<li>
<p><span class="arithmatex"><span class="MathJax_Preview">\boldsymbol{\lambda}^{(0)} = 1</span><script type="math/tex">\boldsymbol{\lambda}^{(0)} = 1</script></span> (vector of ones)</p>
</li>
<li>
<p><strong>Iterate for <span class="arithmatex"><span class="MathJax_Preview">t = 1, 2, \dots, T</span><script type="math/tex">t = 1, 2, \dots, T</script></span> (number of iterations):</strong></p>
</li>
</ol>
<p><strong>Step 1:</strong> <strong>Update <span class="arithmatex"><span class="MathJax_Preview">\beta^{(t)}</span><script type="math/tex">\beta^{(t)}</script></span> (regression coefficients):</strong><br />
   Sample <span class="arithmatex"><span class="MathJax_Preview">\beta^{(t)}</span><script type="math/tex">\beta^{(t)}</script></span> from the conditional posterior:
   $$
   \beta^{(t)} \mid \mathbf{y}, \mathbf{X}, h^{(t-1)}, \Omega^{(t-1)}, \nu_\lambda^{(t-1)} \sim N(\overline \beta,\overline V)
   $$</p>
<p><strong>Step 2:</strong> <strong>Update <span class="arithmatex"><span class="MathJax_Preview">h^{(t)}</span><script type="math/tex">h^{(t)}</script></span> (variance hyperparameter):</strong><br />
   Sample <span class="arithmatex"><span class="MathJax_Preview">h^{(t)}</span><script type="math/tex">h^{(t)}</script></span> from the conditional posterior:
   $$
   h^{(t)} \mid \mathbf{y}, \mathbf{X}, \beta^{(t)}, \nu_\lambda^{(t-1)} \sim  G\left(\overline{s}^{-2}, \overline{\nu}\right)
   $$</p>
<p><strong>Step 3:</strong> <strong>Update <span class="arithmatex"><span class="MathJax_Preview">\boldsymbol{\lambda}^{(t)}, \Omega^{(t)}</span><script type="math/tex">\boldsymbol{\lambda}^{(t)}, \Omega^{(t)}</script></span> (heteroskedasticity parameters):</strong><br />
   For each <span class="arithmatex"><span class="MathJax_Preview">i = 1, \dots, N</span><script type="math/tex">i = 1, \dots, N</script></span>, sample:
   $$
   \lambda_i^{(t)} \mid \mathbf{y}, \mathbf{X}, \beta^{(t)}, h^{(t)}, \nu_\lambda^{(t-1)} \sim f_G\bigg( \lambda_i \big| \frac{\nu_\lambda + 1}{h \epsilon_i^2 + \nu_\lambda}, \nu_\lambda +1  \bigg)
   $$</p>
<p><strong>Step 4:</strong> <strong>Update <span class="arithmatex"><span class="MathJax_Preview">\Omega^{(t)}</span><script type="math/tex">\Omega^{(t)}</script></span> (diagonal precision matrix):</strong><br />
   Set:
   $$
   \nu_\lambda^{(t)} \mid \mathbf{y}, \mathbf{X}, \beta^{(t)}, h^{(t)}, \boldsymbol{\lambda}^{(t)}, \nu_\lambda^{(t-1)} \sim p(\nu_\lambda | y, \beta, h, \lambda)
   $$</p>
<ol>
<li><strong>Repeat steps 1 through 4</strong> until (distributional) convergence or for <span class="arithmatex"><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span> iterations.</li>
</ol>
<p>Let&rsquo;s also include an argument which switches off heteroskedasticity of the model.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">gibbs_sampler</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">priors</span><span class="p">,</span> <span class="n">heteroskedastic</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">draws</span><span class="o">=</span><span class="mi">10_000</span><span class="p">,</span> <span class="n">burn_in</span><span class="o">=</span><span class="mi">1_000</span><span class="p">,</span> <span class="n">MH_params</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;sigma&#39;</span><span class="p">:</span> <span class="mf">.25</span><span class="p">}):</span>
    <span class="n">MH_sigma</span> <span class="o">=</span> <span class="n">MH_params</span><span class="p">[</span><span class="s1">&#39;sigma&#39;</span><span class="p">]</span>
    <span class="n">N</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">gls_dict</span> <span class="o">=</span> <span class="n">gls_dict_wrapper</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]</span><span class="o">*</span><span class="n">N</span><span class="p">))</span>

    <span class="c1"># initialize the parameters</span>
    <span class="n">gibbs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;betas&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">gls_dict</span><span class="p">[</span><span class="s1">&#39;beta&#39;</span><span class="p">]],</span>
        <span class="s1">&#39;h&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="o">/</span><span class="n">gls_dict</span><span class="p">[</span><span class="s1">&#39;s_sq&#39;</span><span class="p">]],</span>
        <span class="s1">&#39;Ps&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]</span><span class="o">*</span><span class="n">N</span><span class="p">)],</span>
        <span class="s1">&#39;nu_lambda&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">priors</span><span class="p">[</span><span class="s1">&#39;nu_lambda&#39;</span><span class="p">]],</span> <span class="c1"># λ: prior mean dof,</span>
        <span class="s1">&#39;mh_acceptances&#39;</span><span class="p">:</span> <span class="p">[]</span> <span class="c1"># counts the number of accepted proposals in the M-H step</span>
    <span class="p">}</span>
    <span class="n">P_root_sample</span> <span class="o">=</span> <span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;Ps&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mf">0.5</span> <span class="c1"># initialize the square root of precision matrix</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">draws</span><span class="o">+</span><span class="n">burn_in</span><span class="p">):</span>

        <span class="c1"># sample beta given h, Omega, ν_λ</span>
        <span class="n">beta_sample</span> <span class="o">=</span> <span class="n">sample_beta</span><span class="p">(</span>
                                <span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;h&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>      <span class="c1"># h: sampled precision</span>
                                <span class="n">P_root_sample</span><span class="p">,</span>       <span class="c1"># λ: (root of) last precision sample</span>
                                <span class="n">priors</span><span class="p">[</span><span class="s1">&#39;beta&#39;</span><span class="p">],</span>      <span class="c1"># β: prior mean</span>
                                <span class="n">priors</span><span class="p">[</span><span class="s1">&#39;V_inv&#39;</span><span class="p">],</span>     <span class="c1"># β: prior precision matrix</span>
                                <span class="n">X</span><span class="p">,</span>
                                <span class="n">y</span><span class="p">)</span>
        <span class="n">resid</span> <span class="o">=</span> <span class="n">get_res</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">beta_sample</span><span class="p">)</span>
        <span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;betas&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">beta_sample</span><span class="p">)</span>

        <span class="c1"># sample h given beta, Omega, ν_λ</span>
        <span class="n">h_sample</span> <span class="o">=</span> <span class="n">sample_h</span><span class="p">(</span><span class="n">P_root_sample</span><span class="p">,</span>      <span class="c1"># λ: (root of) last precision sample</span>
                            <span class="n">resid</span><span class="p">,</span>              <span class="c1"># β: residuals from sampled beta </span>
                            <span class="n">priors</span><span class="p">[</span><span class="s1">&#39;nu&#39;</span><span class="p">],</span>       <span class="c1"># h: prior dof</span>
                            <span class="n">priors</span><span class="p">[</span><span class="s1">&#39;s_sq&#39;</span><span class="p">])</span>     <span class="c1"># h: prior</span>
        <span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;h&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">h_sample</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">heteroskedastic</span><span class="p">:</span>
            <span class="k">continue</span>

        <span class="c1"># sample ν_λ given beta, h, Omega</span>
        <span class="n">nu_lambda_sample</span><span class="p">,</span> <span class="n">mh_acceptance</span> <span class="o">=</span> <span class="n">sample_nu_lambda_new</span><span class="p">(</span><span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;nu_lambda&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> 
                                                               <span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;Ps&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> 
                                                               <span class="n">priors</span><span class="p">[</span><span class="s1">&#39;nu_lambda&#39;</span><span class="p">],</span> 
                                                               <span class="n">MH_sigma</span><span class="p">)</span>
        <span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;nu_lambda&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nu_lambda_sample</span><span class="p">)</span>
        <span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;mh_acceptances&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mh_acceptance</span><span class="p">)</span>

        <span class="c1"># sample lambdas/Omega given beta, h, ν_λ</span>
        <span class="n">P_sample</span> <span class="o">=</span> <span class="n">sample_P</span><span class="p">(</span><span class="n">resid</span><span class="p">,</span>                      <span class="c1"># β: residuals from sampled beta </span>
                                <span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;h&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>         <span class="c1"># h: sampled precision</span>
                                <span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;nu_lambda&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="c1"># λ: last dof sample</span>
        <span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;Ps&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">P_sample</span><span class="p">)</span> 
        <span class="n">P_root_sample</span> <span class="o">=</span> <span class="n">P_sample</span><span class="o">**</span><span class="mf">0.5</span>

    <span class="c1"># discard burn-in</span>
    <span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;betas&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;betas&#39;</span><span class="p">][</span><span class="n">burn_in</span><span class="p">:]</span>
    <span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;h&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;h&#39;</span><span class="p">][</span><span class="n">burn_in</span><span class="p">:]</span>
    <span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;Ps&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;Ps&#39;</span><span class="p">][</span><span class="n">burn_in</span><span class="p">:]</span>
    <span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;nu_lambda&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;nu_lambda&#39;</span><span class="p">][</span><span class="n">burn_in</span><span class="p">:]</span>
    <span class="k">return</span> <span class="n">gibbs</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="c1"># priors (hyperparameters)</span>
<span class="n">K</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">priors</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;beta&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">K</span><span class="p">),</span>        <span class="c1"># beta: zero mean for beta</span>
    <span class="s1">&#39;V_inv&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">K</span><span class="p">)</span><span class="o">*</span><span class="mf">1E-5</span><span class="p">,</span>    <span class="c1"># beta: weak prior for beta</span>
    <span class="s1">&#39;s_sq&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>                  <span class="c1"># h:    expected precision is 1 = 1/s_sq</span>
    <span class="s1">&#39;nu&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>                    <span class="c1"># h:    dof</span>
    <span class="s1">&#39;nu_lambda&#39;</span><span class="p">:</span> <span class="mi">25</span>              <span class="c1"># λ:    prior mean dof</span>
<span class="p">}</span>

<span class="o">%</span><span class="n">timeit</span> <span class="n">gibbs_sampler</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">priors</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="mi">1_000</span><span class="p">,</span> <span class="n">burn_in</span><span class="o">=</span><span class="mi">1_000</span><span class="p">,</span> <span class="n">MH_params</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;sigma&#39;</span><span class="p">:</span> <span class="mf">.5</span><span class="p">})</span>
<span class="o">%</span><span class="n">timeit</span> <span class="n">gibbs_sampler</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">priors</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="mi">10_000</span><span class="p">,</span> <span class="n">burn_in</span><span class="o">=</span><span class="mi">1_000</span><span class="p">,</span> <span class="n">MH_params</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;sigma&#39;</span><span class="p">:</span> <span class="mf">.5</span><span class="p">})</span>
<span class="o">%</span><span class="n">timeit</span> <span class="n">gibbs_sampler</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">priors</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">burn_in</span><span class="o">=</span><span class="mi">1_000</span><span class="p">,</span> <span class="n">MH_params</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;sigma&#39;</span><span class="p">:</span> <span class="mf">.5</span><span class="p">})</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>303 ms ± 51.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
1.41 s ± 71.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
12.4 s ± 308 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
</code></pre></div>
<h3 id="parallel-gibbs-sampler"><strong>Parallel Gibbs Sampler</strong><a class="headerlink" href="#parallel-gibbs-sampler" title="Permanent link">#</a></h3>
<p>To speed things up and get a little practice in parallel computing, let&rsquo;s parallelize the Gibbs sampler using <code>joblib</code>.</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">joblib</span> <span class="kn">import</span> <span class="n">Parallel</span><span class="p">,</span> <span class="n">delayed</span>
<span class="k">def</span> <span class="nf">draw_gibbs_sample</span><span class="p">(</span><span class="n">gibbs</span><span class="p">,</span> <span class="n">priors</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">heteroskedastic</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">mh_params</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;sigma&#39;</span><span class="p">:</span> <span class="mf">.25</span><span class="p">}):</span>
    <span class="n">MH_sigma</span> <span class="o">=</span> <span class="n">mh_params</span><span class="p">[</span><span class="s1">&#39;sigma&#39;</span><span class="p">]</span>
    <span class="n">P_root</span> <span class="o">=</span> <span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;Ps&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mf">0.5</span> <span class="c1"># precision matrix</span>

    <span class="c1"># sample beta given h, Omega, ν_λ</span>
    <span class="n">beta_sample</span> <span class="o">=</span> <span class="n">sample_beta</span><span class="p">(</span>
                            <span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;h&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>      <span class="c1"># h: sampled precision</span>
                            <span class="n">P_root</span><span class="p">,</span>              <span class="c1"># λ: (root of) last precision sample</span>
                            <span class="n">priors</span><span class="p">[</span><span class="s1">&#39;beta&#39;</span><span class="p">],</span>      <span class="c1"># β: prior mean</span>
                            <span class="n">priors</span><span class="p">[</span><span class="s1">&#39;V_inv&#39;</span><span class="p">],</span>     <span class="c1"># β: prior precision matrix</span>
                            <span class="n">X</span><span class="p">,</span>
                            <span class="n">y</span><span class="p">)</span>
    <span class="n">resid</span> <span class="o">=</span> <span class="n">get_res</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">beta_sample</span><span class="p">)</span>

    <span class="c1"># sample h given beta, Omega, ν_λ</span>
    <span class="n">h_sample</span> <span class="o">=</span> <span class="n">sample_h</span><span class="p">(</span><span class="n">P_root</span><span class="p">,</span>      <span class="c1"># λ: (root of) last precision sample</span>
                        <span class="n">resid</span><span class="p">,</span>              <span class="c1"># β: residuals from sampled beta </span>
                        <span class="n">priors</span><span class="p">[</span><span class="s1">&#39;nu&#39;</span><span class="p">],</span>       <span class="c1"># h: prior dof</span>
                        <span class="n">priors</span><span class="p">[</span><span class="s1">&#39;s_sq&#39;</span><span class="p">])</span>     <span class="c1"># h: prior</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">heteroskedastic</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">beta_sample</span><span class="p">,</span> <span class="n">h_sample</span><span class="p">,</span> <span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;Ps&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;nu_lambda&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">0</span>

    <span class="c1"># sample ν_λ given beta, h, Omega</span>
    <span class="n">nu_lambda_sample</span><span class="p">,</span> <span class="n">mh_acceptance</span> <span class="o">=</span> <span class="n">sample_nu_lambda_new</span><span class="p">(</span><span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;nu_lambda&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> 
                                                            <span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;Ps&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> 
                                                            <span class="n">priors</span><span class="p">[</span><span class="s1">&#39;nu_lambda&#39;</span><span class="p">],</span> 
                                                            <span class="n">MH_sigma</span><span class="p">)</span>

    <span class="c1"># sample lambdas/Omega given beta, h, ν_λ</span>
    <span class="n">P_sample</span> <span class="o">=</span> <span class="n">sample_P</span><span class="p">(</span><span class="n">resid</span><span class="p">,</span>                      <span class="c1"># β: residuals from sampled beta </span>
                        <span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;h&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>         <span class="c1"># h: sampled precision</span>
                        <span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;nu_lambda&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="c1"># λ: last dof sample</span>
    <span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;Ps&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">P_sample</span><span class="p">)</span> 

    <span class="k">return</span> <span class="n">beta_sample</span><span class="p">,</span> <span class="n">h_sample</span><span class="p">,</span> <span class="n">P_sample</span><span class="p">,</span> <span class="n">nu_lambda_sample</span><span class="p">,</span> <span class="n">mh_acceptance</span>

<span class="k">def</span> <span class="nf">merge_dictionaries</span><span class="p">(</span><span class="n">dict_list</span><span class="p">):</span>
    <span class="n">master_dict</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">dict_list</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">d</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">master_dict</span><span class="p">:</span>
                <span class="n">master_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">master_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>  <span class="c1"># Unpack the list and append the elements</span>

    <span class="k">return</span> <span class="n">master_dict</span>

<span class="k">def</span> <span class="nf">gibbs_sampler_parallel</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">priors</span><span class="p">,</span> <span class="n">heteroskedastic</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="mi">10_000</span><span class="p">,</span> <span class="n">burn_in</span><span class="o">=</span><span class="mi">1_000</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">mh_params</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;sigma&#39;</span><span class="p">:</span> <span class="mf">.25</span><span class="p">}):</span>
    <span class="n">N</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">ols</span> <span class="o">=</span> <span class="n">gls_dict_wrapper</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]</span><span class="o">*</span><span class="n">N</span><span class="p">))</span>

    <span class="c1"># initialize the parameters</span>
    <span class="n">gibbs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;betas&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">ols</span><span class="p">[</span><span class="s1">&#39;beta&#39;</span><span class="p">]],</span>
        <span class="s1">&#39;h&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="o">/</span><span class="n">ols</span><span class="p">[</span><span class="s1">&#39;s_sq&#39;</span><span class="p">]],</span>
        <span class="s1">&#39;Ps&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]</span><span class="o">*</span><span class="n">N</span><span class="p">)],</span>
        <span class="s1">&#39;nu_lambda&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">priors</span><span class="p">[</span><span class="s1">&#39;nu_lambda&#39;</span><span class="p">]],</span> <span class="c1"># λ: prior mean dof,</span>
        <span class="s1">&#39;mh_acceptances&#39;</span><span class="p">:</span> <span class="p">[]</span> <span class="c1"># counts the number of accepted proposals in the M-H step</span>
    <span class="p">}</span>

    <span class="c1"># burn in</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">burn_in</span><span class="p">):</span>
        <span class="n">beta_sample</span><span class="p">,</span> <span class="n">h_sample</span><span class="p">,</span> <span class="n">P_sample</span><span class="p">,</span> <span class="n">nu_lambda_sample</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">draw_gibbs_sample</span><span class="p">(</span><span class="n">gibbs</span><span class="p">,</span> <span class="n">priors</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">heteroskedastic</span><span class="p">,</span> <span class="n">mh_params</span><span class="p">)</span>

        <span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;betas&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">beta_sample</span><span class="p">)</span>
        <span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;h&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">h_sample</span><span class="p">)</span>
        <span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;Ps&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">P_sample</span><span class="p">)</span>
        <span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;nu_lambda&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nu_lambda_sample</span><span class="p">)</span>

    <span class="c1"># discard burn-in</span>
    <span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;betas&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;betas&#39;</span><span class="p">][</span><span class="n">burn_in</span><span class="p">:]</span>
    <span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;h&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;h&#39;</span><span class="p">][</span><span class="n">burn_in</span><span class="p">:]</span>
    <span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;Ps&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;Ps&#39;</span><span class="p">][</span><span class="n">burn_in</span><span class="p">:]</span>

    <span class="c1"># draw samples</span>
    <span class="n">draws_per_job</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">draws</span> <span class="o">/</span> <span class="n">n_jobs</span><span class="p">))</span>
    <span class="k">def</span> <span class="nf">draw_samples_parallel</span><span class="p">(</span><span class="n">gibbs</span><span class="p">):</span>
        <span class="n">gibbs_local</span> <span class="o">=</span> <span class="n">gibbs</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">draws_per_job</span><span class="p">):</span>
            <span class="n">beta_sample</span><span class="p">,</span> <span class="n">h_sample</span><span class="p">,</span> <span class="n">P_sample</span><span class="p">,</span> <span class="n">nu_lambda_sample</span><span class="p">,</span> <span class="n">mh_acceptance</span> <span class="o">=</span> <span class="n">draw_gibbs_sample</span><span class="p">(</span><span class="n">gibbs_local</span><span class="p">,</span> <span class="n">priors</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">heteroskedastic</span><span class="p">,</span> <span class="n">mh_params</span><span class="p">)</span> 
            <span class="n">gibbs_local</span><span class="p">[</span><span class="s1">&#39;betas&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">beta_sample</span><span class="p">)</span>
            <span class="n">gibbs_local</span><span class="p">[</span><span class="s1">&#39;h&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">h_sample</span><span class="p">)</span>
            <span class="n">gibbs_local</span><span class="p">[</span><span class="s1">&#39;Ps&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">P_sample</span><span class="p">)</span>
            <span class="n">gibbs_local</span><span class="p">[</span><span class="s1">&#39;nu_lambda&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nu_lambda_sample</span><span class="p">)</span>
            <span class="n">gibbs_local</span><span class="p">[</span><span class="s1">&#39;mh_acceptances&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mh_acceptance</span><span class="p">)</span>
        <span class="n">gibbs_local</span><span class="p">[</span><span class="s1">&#39;betas&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># remove the first element (burn in element)</span>
        <span class="n">gibbs_local</span><span class="p">[</span><span class="s1">&#39;h&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">gibbs_local</span><span class="p">[</span><span class="s1">&#39;Ps&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">gibbs_local</span><span class="p">[</span><span class="s1">&#39;nu_lambda&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">gibbs_local</span>

    <span class="n">results</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">)(</span><span class="n">delayed</span><span class="p">(</span><span class="n">draw_samples_parallel</span><span class="p">)(</span><span class="n">gibbs</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_jobs</span><span class="p">))</span>
    <span class="n">gibbs</span> <span class="o">=</span> <span class="n">merge_dictionaries</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">gibbs</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="c1"># priors (hyperparameters)</span>
<span class="n">K</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">priors</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;beta&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">K</span><span class="p">),</span>        <span class="c1"># beta: zero mean for beta</span>
    <span class="s1">&#39;V_inv&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">K</span><span class="p">)</span><span class="o">*</span><span class="mf">1E-5</span><span class="p">,</span>    <span class="c1"># beta: weak prior for beta</span>
    <span class="s1">&#39;s_sq&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>                  <span class="c1"># h:    expected precision is 1 = 1/s_sq</span>
    <span class="s1">&#39;nu&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>                    <span class="c1"># h:    dof</span>
    <span class="s1">&#39;nu_lambda&#39;</span><span class="p">:</span> <span class="mi">25</span>              <span class="c1"># λ:    prior mean dof</span>
<span class="p">}</span>

<span class="n">gibbs</span> <span class="o">=</span> <span class="n">gibbs_sampler_parallel</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">priors</span><span class="p">,</span> <span class="n">heteroskedastic</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="mi">10_000</span><span class="p">,</span> <span class="n">burn_in</span><span class="o">=</span><span class="mi">50_000</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">mh_params</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;sigma&#39;</span><span class="p">:</span> <span class="mf">.5</span><span class="p">})</span>
<span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;mh_acceptances&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;nu_lambda&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="c1">#if heteroskedastic=False, the acceptance rate is 0 and nu_lambda has no effect </span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>(0.0, 25.0)
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="o">%</span><span class="n">time</span> <span class="n">gibbs</span> <span class="o">=</span> <span class="n">gibbs_sampler_parallel</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">priors</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="mi">1_000_000</span><span class="p">,</span> <span class="n">burn_in</span><span class="o">=</span><span class="mi">1_000</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">mh_params</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;sigma&#39;</span><span class="p">:</span> <span class="mf">.5</span><span class="p">})</span>
<span class="o">%</span><span class="n">time</span> <span class="n">gibbs</span> <span class="o">=</span> <span class="n">gibbs_sampler_parallel</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">priors</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="mi">1_000_000</span><span class="p">,</span> <span class="n">burn_in</span><span class="o">=</span><span class="mi">1_000</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">mh_params</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;sigma&#39;</span><span class="p">:</span> <span class="mf">.5</span><span class="p">})</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>CPU times: user 9.87 s, sys: 18.7 s, total: 28.6 s
Wall time: 1min 15s
CPU times: user 11.5 s, sys: 44.7 s, total: 56.2 s
Wall time: 2min 3s
</code></pre></div>
<h4 id="visualization"><strong>Visualization</strong><a class="headerlink" href="#visualization" title="Permanent link">#</a></h4>
<p>We can now e.g. plot the joint distribution of the <span class="arithmatex"><span class="MathJax_Preview">(\beta_1, \beta_2)</span><script type="math/tex">(\beta_1, \beta_2)</script></span>. Also visualize the posterior probability that <span class="arithmatex"><span class="MathJax_Preview">\beta_2 &lt; \beta_1</span><script type="math/tex">\beta_2 < \beta_1</script></span></p>
<div class="highlight"><pre><span></span><code><span class="c1"># take some samples</span>
<span class="n">betas_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;betas&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">10_000</span><span class="p">:])</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="c1"># lets plot the simulated joint distribution of the first two betas</span>
<span class="nb">bool</span> <span class="o">=</span> <span class="n">betas_samples</span><span class="p">[</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.6</span><span class="o">*</span><span class="n">betas_samples</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">betas_samples</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">betas_samples</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Simulated posterior distribution of beta&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\beta_1$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\beta_2$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;prob = &quot;</span><span class="p">,</span> <span class="nb">bool</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
</code></pre></div>
<p><img alt="png" src="../BayReg_files/BayReg_43_0.png" /></p>
<div class="highlight"><pre><span></span><code>prob =  0.001
</code></pre></div>
<h3 id="a-full-class-bayesianregressioninghet"><strong>A Full Class <code>BayesianRegressionIngHet</code></strong><a class="headerlink" href="#a-full-class-bayesianregressioninghet" title="Permanent link">#</a></h3>
<p>Now that the Gibbs sampler is completed, we should package up our functions and write a class for the independent NG prior with heteroskedasticity. We can then deploy the code easily for estimation problems.</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">BayesianRegressionIngHet</span><span class="p">():</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">priors</span><span class="p">,</span> <span class="n">heteroskedastic</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">y</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">priors</span> <span class="o">=</span> <span class="n">priors</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gibbs</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">heteroskedastic</span> <span class="o">=</span> <span class="n">heteroskedastic</span>

    <span class="k">def</span> <span class="nf">run_gibbs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="mi">10_000</span><span class="p">,</span> <span class="n">burn_in</span><span class="o">=</span><span class="mi">1_000</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">mh_params</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;sigma&#39;</span><span class="p">:</span> <span class="mf">1.</span><span class="p">}):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mh_params</span> <span class="o">=</span> <span class="n">mh_params</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gibbs</span> <span class="o">=</span> <span class="n">gibbs_sampler_parallel</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">priors</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="n">draws</span><span class="p">,</span> <span class="n">burn_in</span><span class="o">=</span><span class="n">burn_in</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">heteroskedastic</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">heteroskedastic</span><span class="p">,</span> <span class="n">mh_params</span><span class="o">=</span><span class="n">mh_params</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_posterior_stats</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_posterior_stats</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">gibbs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Run Gibbs sampler first!&quot;</span>
        <span class="n">betas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;betas&#39;</span><span class="p">])</span>
        <span class="n">betas_mean</span> <span class="o">=</span> <span class="n">betas</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">betas_cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">betas</span><span class="p">,</span> <span class="n">rowvar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">h_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;h&#39;</span><span class="p">])</span>
        <span class="n">nu_lambda_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;nu_lambda&#39;</span><span class="p">])</span>
        <span class="n">nu_lambda_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;nu_lambda&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">posterior_stats</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;betas_mean&#39;</span><span class="p">:</span> <span class="n">betas_mean</span><span class="p">,</span>
            <span class="s1">&#39;betas_cov&#39;</span><span class="p">:</span> <span class="n">betas_cov</span><span class="p">,</span>
            <span class="s1">&#39;h_mean&#39;</span><span class="p">:</span> <span class="n">h_mean</span><span class="p">,</span>
            <span class="s1">&#39;nu_lambda_mean&#39;</span><span class="p">:</span> <span class="n">nu_lambda_mean</span><span class="p">,</span>
            <span class="s1">&#39;nu_lambda_var&#39;</span><span class="p">:</span> <span class="n">nu_lambda_var</span>
        <span class="p">}</span>

        <span class="k">return</span> <span class="n">betas_mean</span><span class="p">,</span> <span class="n">betas_cov</span>

    <span class="k">def</span> <span class="nf">summarize_posterior</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">means</span><span class="p">,</span> <span class="n">sds</span>  <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">posterior_stats</span><span class="p">[</span><span class="s1">&#39;betas_mean&#39;</span><span class="p">],</span> <span class="n">mod</span><span class="o">.</span><span class="n">posterior_stats</span><span class="p">[</span><span class="s1">&#39;betas_cov&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">diagonal</span><span class="p">()</span><span class="o">**</span><span class="mf">0.5</span>
        <span class="n">means</span>       <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">means</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">posterior_stats</span><span class="p">[</span><span class="s1">&#39;nu_lambda_mean&#39;</span><span class="p">]])</span>
        <span class="n">sds</span>         <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sds</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">posterior_stats</span><span class="p">[</span><span class="s1">&#39;nu_lambda_var&#39;</span><span class="p">]</span><span class="o">**</span><span class="mf">0.5</span><span class="p">])</span>

        <span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;β</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">means</span><span class="p">))]</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;ν_λ&#39;</span><span class="p">]</span>

        <span class="c1"># Print the header</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;coef&#39;</span><span class="si">:</span><span class="s2">&lt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;val&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;sd&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># Print each row</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">sd</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">means</span><span class="p">,</span> <span class="n">sds</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">:</span><span class="s2">&lt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">mean</span><span class="si">:</span><span class="s2">&gt;10.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">sd</span><span class="si">:</span><span class="s2">&gt;10.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;M-H acceptance rate:&quot;</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;mh_acceptances&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">0.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<h2 id="applications"><strong>Applications</strong><a class="headerlink" href="#applications" title="Permanent link">#</a></h2>
<h3 id="house-prices"><strong>House Prices</strong><a class="headerlink" href="#house-prices" title="Permanent link">#</a></h3>
<p>We use data supplied on the <a href="https://web.ics.purdue.edu/~jltobias/HPRICE.TXT">textbook website</a>. We investigate the degree of heteroskedasticity by Gibbs sampling <span class="arithmatex"><span class="MathJax_Preview">\nu_\lambda</span><script type="math/tex">\nu_\lambda</script></span>. Note that this regression model is somewhat equivalent to a linear regression with fat-tailed, <span class="arithmatex"><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span>-distributed errors with <span class="arithmatex"><span class="MathJax_Preview">\nu_\lambda</span><script type="math/tex">\nu_\lambda</script></span> degrees of freedom. That is exactly the model you would get if you integrate out <span class="arithmatex"><span class="MathJax_Preview">\lambda_i</span><script type="math/tex">\lambda_i</script></span>. Thus, Gibbs sampling <span class="arithmatex"><span class="MathJax_Preview">\nu_\lambda</span><script type="math/tex">\nu_\lambda</script></span> allows us to evaluate the fat-tailedness of our data.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="c1"># URL of the data</span>
<span class="n">url</span> <span class="o">=</span> <span class="s1">&#39;https://web.ics.purdue.edu/~jltobias/HPRICE.TXT&#39;</span>
<span class="c1"># Read the data into a pandas DataFrame</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s1">&#39;\s+&#39;</span><span class="p">)</span>
<span class="c1"># turn data into numpy arrays</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;%price&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;lot_siz&#39;</span><span class="p">,</span> <span class="s1">&#39;#bed&#39;</span><span class="p">,</span> <span class="s1">&#39;#bath&#39;</span><span class="p">,</span> <span class="s1">&#39;#stor&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">X</span><span class="p">))</span>  <span class="c1"># add intercept</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>%price</th>
      <th>lot_siz</th>
      <th>#bed</th>
      <th>#bath</th>
      <th>#stor</th>
      <th>drive.</th>
      <th>recroom</th>
      <th>b-ment</th>
      <th>gas</th>
      <th>aircond</th>
      <th>#gar</th>
      <th>desireloc</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>42000</td>
      <td>5850</td>
      <td>3</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>38500</td>
      <td>4000</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>49500</td>
      <td>3060</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>60500</td>
      <td>6650</td>
      <td>3</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>61000</td>
      <td>6360</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span></span><code><span class="c1"># fix some priors</span>
<span class="n">priors</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;beta&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span>
                      <span class="mi">10</span><span class="p">,</span>
                      <span class="mi">5_000</span><span class="p">,</span>
                      <span class="mi">10_000</span><span class="p">,</span>
                      <span class="mi">10_000</span><span class="p">]),</span> <span class="c1"># beta: zero mean for beta</span>
    <span class="s1">&#39;V_inv&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">10_000</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2_500</span><span class="p">,</span> <span class="mi">5_000</span><span class="p">,</span> <span class="mi">5_000</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)),</span>     <span class="c1"># beta: weak prior for beta USE THE PRIOR SUPPLIED IN THE BOOK</span>
    <span class="s1">&#39;s_sq&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="o">/</span><span class="mi">4</span><span class="o">*</span><span class="mf">1E8</span><span class="p">,</span>            <span class="c1"># h:    expected precision is 1 = 1/s_sq</span>
    <span class="s1">&#39;nu&#39;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>                    <span class="c1"># h:    dof</span>
    <span class="s1">&#39;nu_lambda&#39;</span><span class="p">:</span> <span class="mi">25</span>             <span class="c1"># λ:    hypothesized dof (fat-tailedness of error variance) </span>
<span class="p">}</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">mod</span> <span class="o">=</span> <span class="n">BayesianRegressionIngHet</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">priors</span><span class="p">,</span> <span class="n">heteroskedastic</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">mod</span><span class="o">.</span><span class="n">run_gibbs</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">burn_in</span><span class="o">=</span><span class="mi">25_000</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">mh_params</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;sigma&#39;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">})</span>
<span class="n">mod</span><span class="o">.</span><span class="n">get_posterior_stats</span><span class="p">()</span>
<span class="n">mod</span><span class="o">.</span><span class="n">summarize_posterior</span><span class="p">()</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>coef              val         sd
β1          -457.7392  2907.1692
β2             5.2368     0.3596
β3          2125.2660   966.4355
β4         14917.1624  1652.3078
β5          8121.5951   852.2948
ν_λ            4.4905     1.7443
M-H acceptance rate: 0.4954
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;nu_lambda&#39;</span><span class="p">]),</span> <span class="n">bins</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\nu_\lambda$ Draws&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="../BayReg_files/BayReg_50_0.png" /></p>
<h3 id="is-real-gdp-oscillatory-andor-explosive"><strong>Is Real GDP Oscillatory and/or Explosive?</strong><a class="headerlink" href="#is-real-gdp-oscillatory-andor-explosive" title="Permanent link">#</a></h3>
<p>Geweke (1988, Journal of Business and Economic Statistics)</p>
<p>We investigate whether real GDP oscillates (the lag polynomial has an imaginary root) or is explosive (the lag polynomial has at least one root within the unit cycle) in an <span class="arithmatex"><span class="MathJax_Preview">AR(p)</span><script type="math/tex">AR(p)</script></span> model. The model is </p>
<p>$$ 
    y_t = \beta_1 + \sum_{j=1}^p \beta_{1+j} y_{t-j} + \varepsilon_t,
$$
with independent prior and no heteroskedasticity.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">pandas_datareader.data</span> <span class="k">as</span> <span class="nn">web</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">start</span><span class="p">,</span> <span class="n">end</span> <span class="o">=</span> <span class="s1">&#39;1947-01-01&#39;</span><span class="p">,</span> <span class="s1">&#39;2025-01-01&#39;</span> <span class="c1"># &#39;2005-07-01&#39;</span>

<span class="c1"># Fetch the GDPC1 time series data from FRED</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">web</span><span class="o">.</span><span class="n">DataReader</span><span class="p">(</span><span class="s1">&#39;GDPC1&#39;</span><span class="p">,</span> <span class="s1">&#39;fred&#39;</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">&#39;FRED_API_KEY&#39;</span><span class="p">),</span> <span class="n">start</span><span class="o">=</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="n">end</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;log_GDP&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;GDPC1&#39;</span><span class="p">])</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;log_GDP&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;US Real GDP (log scale)&#39;</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="../BayReg_files/BayReg_52_0.png" /></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Manipulate the data to create a time series regression</span>
<span class="k">def</span> <span class="nf">lagged_time_series_df</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">num_lags</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">num_lags</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_lags</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="o">-</span><span class="n">i</span><span class="p">]</span>
    <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1"># intercept</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">num_lags</span><span class="p">:,</span> <span class="p">:]</span> <span class="c1"># remove the first few rows with NaNs</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">num_lags</span><span class="p">:]</span> <span class="c1"># remove the first few rows with NaNs</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">((</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">)),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;const&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;lag_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_lags</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span> <span class="n">index</span><span class="o">=</span><span class="n">index</span><span class="p">[</span><span class="n">num_lags</span><span class="p">:])</span>
    <span class="k">return</span> <span class="n">df</span>

<span class="n">df_ts</span> <span class="o">=</span> <span class="n">lagged_time_series_df</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;log_GDP&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">(),</span> <span class="mi">3</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
<span class="n">df_ts</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>y</th>
      <th>const</th>
      <th>lag_1</th>
      <th>lag_2</th>
      <th>lag_3</th>
    </tr>
    <tr>
      <th>DATE</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1947-10-01</th>
      <td>7.699141</td>
      <td>1.0</td>
      <td>7.683603</td>
      <td>7.685653</td>
      <td>7.688309</td>
    </tr>
    <tr>
      <th>1948-01-01</th>
      <td>7.714089</td>
      <td>1.0</td>
      <td>7.699141</td>
      <td>7.683603</td>
      <td>7.685653</td>
    </tr>
    <tr>
      <th>1948-04-01</th>
      <td>7.730478</td>
      <td>1.0</td>
      <td>7.714089</td>
      <td>7.699141</td>
      <td>7.683603</td>
    </tr>
    <tr>
      <th>1948-07-01</th>
      <td>7.736207</td>
      <td>1.0</td>
      <td>7.730478</td>
      <td>7.714089</td>
      <td>7.699141</td>
    </tr>
    <tr>
      <th>1948-10-01</th>
      <td>7.737339</td>
      <td>1.0</td>
      <td>7.736207</td>
      <td>7.730478</td>
      <td>7.714089</td>
    </tr>
  </tbody>
</table>
</div>

<p>Now, set up a dictionary for priors and run the Gibbs sampler.</p>
<div class="highlight"><pre><span></span><code><span class="n">priors</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;beta&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span>
                      <span class="mi">0</span><span class="p">,</span>
                      <span class="mi">0</span><span class="p">,</span>
                      <span class="mi">0</span><span class="p">]),</span> <span class="c1"># beta: zero mean for beta</span>
    <span class="s1">&#39;V_inv&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">((</span><span class="mf">1E6</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">))</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)),</span>     <span class="c1"># beta: weak prior</span>
    <span class="s1">&#39;s_sq&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>           <span class="c1"># h:    expected precision is 1 = 1/s_sq</span>
    <span class="s1">&#39;nu&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>             <span class="c1"># h:   </span>
    <span class="s1">&#39;nu_lambda&#39;</span><span class="p">:</span> <span class="mi">1000</span>    <span class="c1"># λ:    dont matter, no heteroskedasticity</span>
<span class="p">}</span>

<span class="n">mod</span> <span class="o">=</span> <span class="n">BayesianRegressionIngHet</span><span class="p">(</span><span class="n">df_ts</span><span class="p">[[</span><span class="s1">&#39;const&#39;</span><span class="p">,</span> <span class="s1">&#39;lag_1&#39;</span><span class="p">,</span> <span class="s1">&#39;lag_2&#39;</span><span class="p">,</span> <span class="s1">&#39;lag_3&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">(),</span> <span class="n">df_ts</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">(),</span> 
                               <span class="n">priors</span><span class="p">,</span> 
                               <span class="n">heteroskedastic</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">mod</span><span class="o">.</span><span class="n">run_gibbs</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">1_000_000</span><span class="p">,</span> <span class="n">burn_in</span><span class="o">=</span><span class="mi">25_000</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">mod</span><span class="o">.</span><span class="n">get_posterior_stats</span><span class="p">()</span>
<span class="n">mod</span><span class="o">.</span><span class="n">summarize_posterior</span><span class="p">()</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>coef              val         sd
β1             0.0248     0.0085
β2             1.1008     0.0571
β3            -0.0206     0.0851
β4            -0.0822     0.0569
ν_λ         1000.0000     0.0000
M-H acceptance rate: 0.0000
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">roots_test</span><span class="p">(</span><span class="n">sample</span><span class="p">):</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">sample</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">b</span> <span class="o">=</span> <span class="o">-</span><span class="n">b</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">b</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">roots</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">roots</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    <span class="n">oscillatory_event</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">imag</span><span class="p">(</span><span class="n">roots</span><span class="p">)</span> <span class="o">!=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">roots</span><span class="p">)))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="mi">2</span>
    <span class="n">out_unit_circle_event</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">roots</span><span class="p">))</span> <span class="o">&lt;</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">oscillatory_event</span><span class="p">,</span> <span class="n">out_unit_circle_event</span>

<span class="k">def</span> <span class="nf">roots_test_parallel</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">)(</span><span class="n">delayed</span><span class="p">(</span><span class="n">roots_test</span><span class="p">)(</span><span class="n">sample</span><span class="p">)</span> <span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">)</span>
    <span class="n">oscillatory_events</span><span class="p">,</span> <span class="n">in_unit_circle_events</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">results</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">oscillatory_events</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">in_unit_circle_events</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">print_probabilities</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">start_date</span><span class="p">,</span> <span class="n">end_date</span><span class="p">,</span> <span class="n">dependent_var_name</span><span class="p">):</span>
    <span class="c1"># Calculate the mean of A and B</span>
    <span class="n">mean_A</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">mean_B</span> <span class="o">=</span> <span class="n">B</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="c1"># Print the sample period and dependent variable name</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Dependent Variable:&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">dependent_var_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Sample Period:&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">start_date</span><span class="si">}</span><span class="s2"> to </span><span class="si">{</span><span class="n">end_date</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>


    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Pr(Y has oscillatory behavior) = </span><span class="si">{</span><span class="n">mean_A</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Pr(Y has explosive behavior)   = </span><span class="si">{</span><span class="n">mean_B</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">A</span><span class="p">,</span> <span class="n">B</span> <span class="o">=</span> <span class="n">roots_test_parallel</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">gibbs</span><span class="p">[</span><span class="s1">&#39;betas&#39;</span><span class="p">],</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">print_probabilities</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="s1">&#39;log real GDP (US)&#39;</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>Dependent Variable:  log real GDP (US)
Sample Period:       1947-01-01 to 2025-01-01

Pr(Y has oscillatory behavior) = 0.0649
Pr(Y has explosive behavior)   = 0.0132
</code></pre></div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.top", "navigation.indexes"], "search": "../../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.471ce7a9.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
  </body>
</html>